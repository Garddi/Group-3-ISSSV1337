---
title: "Final_Report"
output: html_document
date: '2022-08-03'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




## DECISIONS/ VEDTAK 

First get all legislations in the Norwegian Parlament since 2011 with get_session_decisions

```{r, eval=FALSE}
session_storting <- get_parlsessions()

session_storting <- session_storting %>%
  filter(id %in% c("2011-2012", "2012-2013", "2013-2014",
                   "2014-2015", "2015-2016", "2016-2017",
                   "2017-2018", "2018-2019", "2019-2020", 
                   "2020-2021", "2021-2022"))


a<-list()

for(x in unique(session_storting$id)){
   it <- 100*(which(unique(session_storting$id) == x) / length(unique(session_storting$id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  
  a[[x]] <- get_session_decisions(sessionid = x, good_manners = 0)
}
```



## Findig how many times FN-sambandet and their work with the SDG 4,7 "Utdanning for bærekraftig utvikling" are mentioned in legisaltions


```{r}
decisionslista <- do.call("rbind", a)

sum(str_detect(decisionslista$decision_text, "FN-sambandet")) 
# 0 mentions of the spesific string FN-sambandet

sum(str_detect(decisionslista$decision_text, "4,7"))
# 19 mentions of 4,7 

sum(str_detect(decisionslista$decision_text, "utdanning for bærekraftig utvikling"))
# 0 mentions of education for sustainable development

sum(str_detect(decisionslista$decision_text, "utdanning"))
# 291 mentions of education

sum(str_detect(decisionslista$decision_text, "bærekraftig"))
# 82 mentions of sustainable


save(decisionslista, file = "Question_Data/MetadataDecisions.Rdata")
```



## Trying another method 


```{r}
## isolate the decision text
decisions <- decisionslista %>%
  select(case_id, decision_text, decision_title, decision_type_id, decision_type_name)

head(decisions)

decisions <- decisions%>% 
  mutate(text_l = str_to_lower(decision_text))
```


```{r}
text <- decisions %>% 
  select(decision_text) %>% 
  pull()

id <- decisions %>% 
  select(case_id) %>% 
  pull()


head(text)
```


```{r}
text2 <- text %>% 
  str_squish() %>% # Remove whitespace from the start and end of a string, and inside a string
  str_split("\n") %>% # Make a list of strings by splitting them on a certain pattern
  str_to_lower() %>%  # Make all characters into lower case
  str_remove("stortinget ber regjeringen") %>% # Fjerner string som nevnes i hvert vedtak
  str_remove("det henstilles til regjeringen") %>% 
  str_trim() 

head(text2) # here we see that there is now less noise in the text

text_final <- as_tibble(list( text= unlist(text2), id = unlist(id))) # Put the text into a column in a tibble

head(text_final)
```


```{r}

# Subseting the strings that contain a specific pattern, then we can see in which context the strings are mentioned
# NB! do not run if your computer is slow

str_subset(text2, "4,7")
str_subset(text2, "utdanning")
str_subset(text2, "bærekraft")
str_subset(text2, "ubu")

```




### Trying topic modelling


```{r}
decision_tokens <- text_final %>%
unnest_tokens(input = text, # Which variable to get the text from
output = word, # What the new variable should be called
token = "words") # choosing a unigram
decision_tokens
```


```{r}
decision_tokens %>%
count(id, word, sort = TRUE) # Counting the number of words per case and sorting from highest to lowest
```

Here we see that there are a lot of words that do not really tell us anything, 
like "type_inrykk", therefore I also create my own stopwords which will be emitted from the text. 

```{r}

## Creating a stopword dataset
stop_words <- get_stopwords(language = "no")

# Adding my own words
removeword <- data.frame(word = c("type_innrykk", "type_uinnrykk", "type_fri",
                                  "type_head","p",
                                  "class","strtngt_pkt","strtngt_a",
                                  "strtngt_vedtaks",
                                  "strtngt_liste", "div", "td", "align_right",
                                  "align_left", "tr", "span"), lexicon = "own library")

stop_words <- rbind(stop_words, removeword)

```


```{r}
decision_tokens <- decision_tokens %>%
anti_join(stop_words, by = "word") # Joining against the stopwords dataframe to get rid of cells with stopwords
decision_tokens %>%
count(id, word, sort = TRUE)
```


I now see that there are still many words that give no information. 
However, all these words are coming from the same words strings from the beginning
of each case text. Therefore I will try to remove the first text string from each case. 




```{r}
# adding more words
removeword <- data.frame(word = c("000", "160;000", "1", "2", "3", "4", "5", "6", "7",
                                  "br", "a", "b", "stortinget", "strtngt_forslag",
                                  "trtngt_vedtaks", "e", "nr", "type_sentrert", 
                                  "strtngt_uth", "li", "ul", "th", "b",
                                  "type_kursiv", "krnl_wide_letterspacing",
                                  "type_sperret", "krnl_wide_letterspacing", "[0-9]", 
                                  "type_sperret", "romertall"), lexicon = "own library")

stop_words <- rbind(stop_words, removeword)

```


```{r}
decision_tokens <- decision_tokens %>%
anti_join(stop_words, by = "word") # Joining against the stopwords dataframe to get rid of cells with stopwords
decision_tokens %>%
count(id, word, sort = TRUE)


# remove numbers
nums <- decision_tokens %>% filter(str_detect(word, "^[0-9]")) %>% select(word) %>% unique()

decision_tokens <- decision_tokens %>% 
  anti_join(nums, by = "word")

```


# Stemming the words

```{r}
decision_tokens <- decision_tokens %>%
mutate(stem = wordStem(word))
```


## Plotting 

```{r, eval=FALSE}

plot_d <- decision_tokens %>% # Make a new dataframe to plot
count(id, stem) %>% # Counting the number of words per case
group_by(id) %>% # Grouping by case and...
slice_max(n, n = 5) %>% # Taking out the five top most used words per case
ungroup() # Ungrouping to get the dataframe back to normal


# IKKE KJØR, GIR IKKE godt nok output

plot_d %>%
ggplot(aes(n, fct_reorder(stem, n), # Placing number of occurences (n) on the x-axis and word on the y-axis
fill = id)) + # Adding colors to the bars after the songname
geom_bar(stat = "identity") + # Making a barplot where the y-axis is the measure (stat = "identity")
facet_wrap(~ id, ncol = 4, scales = "free") + # Making one small plot per songname, arranging them in three columns and let the y-axis vary independent of the other plots
labs(x = "", y = "") + # Removing all labels on x- and y-axis
theme_bw() + # Make the background white
theme(legend.position = "none") # Removing the legend for what colors mean

```





```{r}
decisiontokens_dfm <- decision_tokens %>%
  count(id, stem, name = "count") %>% # By default, the count-variable is called "n". Use name = "" to change it.
  cast_dfm(id, # Specify the douments in your analysis (becoming the rows)
           stem, # Specify the tokens in your analysis (becoming the columns)
           count) # Specify the number of times each token shows up in each document (becoming the cells)




```




```{r}
###### --------- Structural Topic Modelling for K = 35 -------------

decisiontokens_lda <- stm(decisiontokens_dfm, # Use a document feature matrix as input
                          init.type = "LDA", # Specify LDA as the type of model
                          K = 35, # Specify the number of topics to estimate
                          seed = 910, # Make the model reproducible - does not work so well :/
                          verbose = FALSE) # Set to FALSE to avoid lots of text while running the model


decisiontopics <- tidy(decisiontokens_lda, 
                       matrix = "beta")


decisiontopics_group <- decisiontopics %>%
  group_by(topic) %>% # Getting the top term per topic, thus using group_by
  slice_max(beta, n = 10) %>% # Fetching the 10 terms with the highest beta
  ungroup() # Ungrouping to get the dataframe back to normal

decisiontopics_group

```



```{r}
decisiontopics_1 <- decisiontopics_group %>% 
  filter(topic %in% c(1:5))

decisiontopics_2 <- decisiontopics_group %>% 
  filter(topic %in% c(5:10))

decisiontopics_3 <- decisiontopics_group %>% 
  filter(topic %in% c(15:20))

decisiontopics_4 <- decisiontopics_group %>% 
  filter(topic %in% c(20:25))

decisiontopics_5 <- decisiontopics_group %>% 
  filter(topic %in% c(25:30))

decisiontopics_6 <- decisiontopics_group %>% 
  filter(topic %in% c(30:35))
```




```{r}
plot_1 <- decisiontopics_1 %>% 
  ggplot(aes(term, beta, fill= topic)) +
  geom_bar(stat= "identity")+
  facet_wrap(~ topic,
             ncol= 3,
             scales= "free") +
  labs(x= "", y="Word Topic Probability") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x= element_text(angle=90, vjust = 0.5, hjust= 1))
  
  
plot_1


```

```{r}
plot_2 <- decisiontopics_2 %>% 
  ggplot(aes(term, beta, fill= topic)) +
  geom_bar(stat= "identity")+
  facet_wrap(~ topic,
             ncol= 3,
             scales= "free") +
  labs(x= "", y="Word Topic Probability") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x= element_text(angle=90, vjust = 0.5, hjust= 1))
  
plot_2

```


```{r}
plot_3 <- decisiontopics_3 %>% 
  ggplot(aes(term, beta, fill= topic)) +
  geom_bar(stat= "identity")+
  facet_wrap(~ topic,
             ncol= 3,
             scales= "free") +
  labs(x= "", y="Word Topic Probability") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x= element_text(angle=90, vjust = 0.5, hjust= 1))
  
plot_3
```



```{r}
plot_4 <- decisiontopics_4 %>% 
  ggplot(aes(term, beta, fill= topic)) +
  geom_bar(stat= "identity")+
  facet_wrap(~ topic,
             ncol= 3,
             scales= "free") +
  labs(x= "", y="Word Topic Probability") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x= element_text(angle=90, vjust = 0.5, hjust= 1))
  
plot_4
```

```{r}
plot_5 <- decisiontopics_5 %>% 
  ggplot(aes(term, beta, fill= topic)) +
  geom_bar(stat= "identity")+
  facet_wrap(~ topic,
             ncol= 3,
             scales= "free") +
  labs(x= "", y="Word Topic Probability") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x= element_text(angle=90, vjust = 0.5, hjust= 1))
  
plot_5
```

```{r}
plot_6 <- decisiontopics_6 %>% 
  ggplot(aes(term, beta, fill= topic)) +
  geom_bar(stat= "identity")+
  facet_wrap(~ topic,
             ncol= 3,
             scales= "free") +
  labs(x= "", y="Word Topic Probability") +
  theme_bw() +
  theme(legend.position = "none",
        axis.text.x= element_text(angle=90, vjust = 0.5, hjust= 1))
  
plot_6
```

```{r}

probabilities <- tidy(decisiontokens_lda, matrix = "gamma", # Calculating document probabilities
                             document_names = rownames(decisiontokens_dfm)) # Adding the cases


top_docs <- probabilities %>%
  group_by(document) %>% # Find the next statistic per document
  slice_max(gamma, n = 3) # Find the max value

topic_assignment <- top_docs %>%
  group_by(document) %>%
  summarise(TopTopic = first(topic),
            SecTopic = nth(topic, 2),
            ThirdTopic = nth(topic, 3),
            TopGamma = first(gamma),
            SecGamma = nth(gamma, 2), 
            ThirdGamma = nth(gamma, 3))

topic_assignment <- topic_assignment %>%
  rename(id = document)

head(topic_assignment)
glimpse(topic_assignment)


```







### TWITTER

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE)
library(rtweet) #using the rtweet package to access the Twitter API
library(httpuv)
library(tidyverse)
```

## Using Twitter´s API to count number of tweets related to FN-sambandet

To get access to the Twitter API you need to apply for a Twitter developers account,
which is a bit tedious.
Then you insert the different keys they give you, or passwords, in the code below. 

```{r, eval=FALSE}

# store api keys (replace with your own - these are fake examples)
api_key <- "000"
api_secret_key <- "ABC"


access_token <- "123abc"
access_token_secret <- "dddd"

# authenticate
token <- create_token(
  app = "thename",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```


When you have done this you can just run this code the next time you want to work with the
rtweet package

```{r}
get_token()
```

## Using search_tweets to find all tweets related to the string FNsambandet
We see that there is only five tweets related to the string FNsambandet

```{r}
#Search tweets related to FNsambandet
twts_fnsambandet <- search_tweets("FNsambandet", n = 1000)


# Create a table of the different tweets
sc_twts <- data.frame(twts_fnsambandet$full_text)
sc_twts

```

Here I try to search for tweets that use the hashtag #FNsambandet, 
however it returns a dataset of 0 obs, which means there are no one who uses the hashtag on Twitter

```{r}
twts_hashtag <- search_tweets("#FNsambandet", 
                           n = 2000)
```



# Searching for the strings "utdanning for bærekratig utvikling" and "ubu", which is part of
FN's Sustainable Development Goal 4, Quality Education, an important part of FN-Sambandet's work.

```{r}
#Search tweets related to FN goal 4,7
tweets_ubu <- search_tweets("utdanning for bærekraftig utvikling", n = 10000)


#Search tweets related to sdg4,7
tweets_sdg <- search_tweets("sdg4,7", n = 1000, lang= "en")

sc_sdg <- table(tweets_sdg$full_text)
sc_sdg

```
























