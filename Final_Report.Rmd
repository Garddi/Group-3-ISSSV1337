---
title: "ISSSV-1337, Group 3 Report UN Association of Norway"
author: "Gard Olav Dietrichson, Hannes Brauer, Markus Opheim, Andreas Kroknes, Ingrid Johannesen, Nora Didriksen, Tia Tiller"
date: '2022-07-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Given Objective

The task provided to us, was to provide the UNA with "quantitative measures" of their work. The wish from our mission provider was to gain knowledge about how much the members of parliament are aware of the activities of the UN


## Problem 

The central problem at hand is how to measure the impact of lobbying, as direct contact between politicians is difficult to find and to measure. 


## Our Approach

In order to map the knowledge of the activities of the UNA, we decided to collect all available texts from Stortinget. After gathering these texts, we needed to figure out how to measure relative prevalence in our data. To this we first decided to create a list over relevant key terms that represent the work of the UNA. We divided these into three levels, based on the suggestions provided to us by our mission provider. The top level search word is "FN-sambandet" the UNA in Norwegian. The first thing that struck us was the fact that this yielded very few results, and as such we saw it necessary to focus much of our work outside what UNA has been explicitly mentioned in. 

Our mission description also contained more suggestions for keywords, and so we used those to create level 2 and level 3 searchwords. Level 2 consists of a particular goal that our provider explained that they had been working towards. This goal was UN sustainability goal 4.7 Education for Sustainable Development. The keywords here were also used to investigate, but again returned very few results. The problems of this will be elaborated on in a later section. 

The final level of search words was the relevant humanitarian sub organisations of UN, such as UNAIDS, WFP, UNDP and UNICEF. These ended up forming the backbone of our approach, as they were far more prevalent, however, all words were maintained in the end, when a complete analysis was performed. 

Finally it is worth mentioning that we also foresaw the large amount of documents being being problematic, as such, we early on decided to limit our timeframe. The package stortingsscrape informs us that voting data is only available from the 2011-2012 session and onwards, so we viewed that as a natural starting point. 

## Sectioning 

Due to the large corpus that would arise from joining all documents, and the severe lack of coherent genre textuality, and to the unique opportunities afforded by each form of retrievable information, we decided to split up our work, and pursue separate roads of inquiry based on each section. 

# Questions 

The questions were scraped, below is the plot drawing the overview of the spreading of mentions of our relevant keywords over the relevant timeframe.

## The questions we ask of the questions

 - Who are asking questions about the UN and the UNA?
 - What are these questions about?
 - What questions mentions the UNA and the associated UN organizations?
 - How are these talked about?
 - Do these questions touch on issues regarding sustainability and other topics of interest that the UNA might be interested in?
 - Other interesting patterns in the questions
 
In order to answer this, we scrape all the questions from the storting's homepage. The following code scrapes all the questions for our relevant time frame. 

```{r, eval=FALSE}
library(tidyverse)
library(stortingscrape)

## For now the sessions we are interested in are limited to 2011-2022

sessions_storting <- get_parlsessions()

sessions_storting <- sessions_storting %>%
  filter(id %in% c("2011-2012", "2012-2013", "2013-2014",
                   "2014-2015", "2015-2016", "2016-2017",
                   "2017-2018", "2018-2019", "2019-2020", 
                   "2020-2021", "2021-2022"))
```

This section retrieves all the sessions that we are interested in, the object `sessions_storting` will be used for most of our code chunks later.

The next step is then to retrieve the metadata on all the questions.

```{r, eval=FALSE}
## The id's are then entered into this loop, which *should* retrieve 
## ALL question id's for our relevant period. 

a<-list()
b<-list()
c<-list()

for(x in unique(sessions_storting$id)){
  a[[x]] <- get_session_questions(sessionid = x, q_type = "interpellasjoner", status = NA, good_manners = 0)
  b[[x]] <- get_session_questions(sessionid = x, q_type = "sporretimesporsmal", status = NA, good_manners = 0)
  c[[x]] <- get_session_questions(sessionid = x, q_type = "skriftligesporsmal", status = NA, good_manners = 0)
  
  #paste0("stortingsporsmal", x) = rbind(a, b, c)
}

## Unlisting all of the resulting lists, they are lists of identically 
## sized dataframes.

questionlista <- do.call("rbind", a)
questionlistb <- do.call("rbind", b)
questionlistc <- do.call("rbind", c)

clist <- list(questionlista, questionlistb, questionlistc)

## Combining all for a dataframe we can fetch 

questionlist <- do.call("rbind", clist)

```

This has created our full list of questions, but not the text of the question and answer themselves. The next step is retrieving all of those. 

```{r, eval = FALSE}

d <- list()

for(x in unique(questionlist$id)){
  it <- 100*(which(unique(questionlist$id) == x) / length(unique(questionlist$id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  
  d[[x]] <- get_question(questionid = x, good_manners = 0)
  #paste0("stortingsporsmal", x) = rbind(a, b, c)
}


questiontext <- do.call("rbind", d)

save(questionlist, file = "Question_Data/MetadataQuestionList.Rdata")
save(questiontext, file = "Question_Data/All_Questions.Rdata")

```

This now gives us a dataframe for analysis. After this we approach the data from two ways, we create a structural topic model, and we use this to analyse the totality of the questions dataframe. 

The structural topic model is created with the code below. First we have to include the answers as well, as part of the documents. 

```{r, eval=FALSE}
answerlist <- questiontext %>%
  select(id, text = answer_text, title, type, question_from_id, 
         qustion_to_id)

## Dropping the observations with no text, note that no text in 
## Stortingscrape is coded as empty string, not as an NA

answerlist <- answerlist %>%
  mutate(notapplicable = ifelse(text == "", NA, 1)) %>%
  drop_na(notapplicable) %>%
  select(-notapplicable)

## Limiting the dataset

questionshort <- questiontext %>%
  select(id, text = question_text, title, type, question_from_id,
         qustion_to_id)

## Binding the sets together, so I have one dataframe with complete
## texts from either answers or questions. **OF NOTE** I should probably
## have added a string to the id's so that i separate answer texts 
## from question texts, given their identical id.

assembledtextquestions <- rbind(questionshort, answerlist)

## removing reduntant objects

rm(answerlist, questionlist, questionshort, questiontext)

## First i turn all strings into lower case

assembledtextquestions <- assembledtextquestions %>% 
  mutate(text_l = str_to_lower(text))

## Before tokenizing I create a stopword dataset, I add br, because it
## is not properly removed from some observations. 

stop_words <- get_stopwords(language = "no")

removeword <- data.frame(word = "br", lexicon = "own library")

stop_words <- rbind(stop_words, removeword)

## Next I tokennize the text, this takes a moment. 

questiontokens <- assembledtextquestions %>%
  unnest_tokens(input = text, # Which variable to get the text from
                output = word, # What the new variable should be called
                token = "words") # Type of tokens to split the text into

## Removing all stopwords from my stop words book

questiontokens <- questiontokens %>%
  anti_join(stop_words, by = "word") # Joining against the stopwords dataframe to get rid of cells with stopwords

## Brief category investigation, stock companies are frequent

questiontokens %>%
  count(id, word, sort = TRUE)

## Creating the document feature matrix

questiontokens <- questiontokens %>%
  mutate(stem = wordStem(word, language = "norwegian"))

questiontokens_dfm <- questiontokens %>%
  count(id, stem, name = "count") %>% # By default, the count-variable is called "n". Use name = "" to change it.
  cast_dfm(id, # Specify the douments in your analysis (becoming the rows)
           stem, # Specify the tokens in your analysis (becoming the columns)
           count) # Specify the number of times each token shows up in each document (becoming the cells)

## Saving these objects, ready for analysis, note that the tokens dataframe
## is far too large to be uploaded to GitHub

save(questiontokens_dfm, file = "Question_Data/questionsdfm.Rdata")
save(questiontokens, file = "Question_Data/questiontokens.Rdata")

```

With the document feature matrix, we can apply an stm function to it. We run it with 75 categories, slightly more than what the package itself reccommends for a corpus of this size.

```{r, eval=FALSE}



```