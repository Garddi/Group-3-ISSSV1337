---
title: "ISSSV-1337, Group 3 Report UN Association of Norway"
author: "Gard Olav Dietrichson, Hannes Brauer, Markus Opheim, Andreas Kroknes, Ingrid Johannesen, Nora Didriksen, Tia Tiller"
date: '2022-07-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Given Objective

The task provided to us, was to provide the UNA with "quantitative measures" of their work. The wish from our mission provider was to gain knowledge about how much the members of parliament are aware of the activities of the UN


## Problem 

The central problem at hand is how to measure the impact of lobbying, as direct contact between politicians is difficult to find and to measure. 


## Our Approach

In order to map the knowledge of the activities of the UNA, we decided to collect all available texts from Stortinget. After gathering these texts, we needed to figure out how to measure relative prevalence in our data. To this we first decided to create a list over relevant key terms that represent the work of the UNA. We divided these into three levels, based on the suggestions provided to us by our mission provider. The top level search word is "FN-sambandet" the UNA in Norwegian. The first thing that struck us was the fact that this yielded very few results, and as such we saw it necessary to focus much of our work outside what UNA has been explicitly mentioned in. 

Our mission description also contained more suggestions for keywords, and so we used those to create level 2 and level 3 searchwords. Level 2 consists of a particular goal that our provider explained that they had been working towards. This goal was UN sustainability goal 4.7 Education for Sustainable Development. The keywords here were also used to investigate, but again returned very few results. The problems of this will be elaborated on in a later section. 

The final level of search words was the relevant humanitarian sub organisations of UN, such as UNAIDS, WFP, UNDP and UNICEF. These ended up forming the backbone of our approach, as they were far more prevalent, however, all words were maintained in the end, when a complete analysis was performed. 

Finally it is worth mentioning that we also foresaw the large amount of documents being being problematic, as such, we early on decided to limit our timeframe. The package stortingsscrape informs us that voting data is only available from the 2011-2012 session and onwards, so we viewed that as a natural starting point. 

## Sectioning 

Due to the large corpus that would arise from joining all documents, and the severe lack of coherent genre textuality, and to the unique opportunities afforded by each form of retrievable information, we decided to split up our work, and pursue separate roads of inquiry based on each section. 

# Questions 

The questions were scraped, below is the plot drawing the overview of the spreading of mentions of our relevant keywords over the relevant timeframe.

## The questions we ask of the questions

 - Who are asking questions about the UN and the UNA?
 - What are these questions about?
 - What questions mentions the UNA and the associated UN organizations?
 - How are these talked about?
 - Do these questions touch on issues regarding sustainability and other topics of interest that the UNA might be interested in?
 - Other interesting patterns in the questions
 
In order to answer this, we scrape all the questions from the storting's homepage. The following code scrapes all the questions for our relevant time frame. 

```{r, eval=FALSE}
library(tidyverse)
library(stortingscrape)

## For now the sessions we are interested in are limited to 2011-2022

sessions_storting <- get_parlsessions()

sessions_storting <- sessions_storting %>%
  filter(id %in% c("2011-2012", "2012-2013", "2013-2014",
                   "2014-2015", "2015-2016", "2016-2017",
                   "2017-2018", "2018-2019", "2019-2020", 
                   "2020-2021", "2021-2022"))
```

This section retrieves all the sessions that we are interested in, the object `sessions_storting` will be used for most of our code chunks later.

The next step is then to retrieve the metadata on all the questions.

```{r, eval=FALSE}
## The id's are then entered into this loop, which *should* retrieve 
## ALL question id's for our relevant period. 

a<-list()
b<-list()
c<-list()

for(x in unique(sessions_storting$id)){
  a[[x]] <- get_session_questions(sessionid = x, q_type = "interpellasjoner", status = NA, good_manners = 0)
  b[[x]] <- get_session_questions(sessionid = x, q_type = "sporretimesporsmal", status = NA, good_manners = 0)
  c[[x]] <- get_session_questions(sessionid = x, q_type = "skriftligesporsmal", status = NA, good_manners = 0)
  
  #paste0("stortingsporsmal", x) = rbind(a, b, c)
}

## Unlisting all of the resulting lists, they are lists of identically 
## sized dataframes.

questionlista <- do.call("rbind", a)
questionlistb <- do.call("rbind", b)
questionlistc <- do.call("rbind", c)

clist <- list(questionlista, questionlistb, questionlistc)

## Combining all for a dataframe we can fetch 

questionlist <- do.call("rbind", clist)

```

This has created our full list of questions, but not the text of the question and answer themselves. The next step is retrieving all of those. 

```{r, eval = FALSE}

d <- list()

for(x in unique(questionlist$id)){
  it <- 100*(which(unique(questionlist$id) == x) / length(unique(questionlist$id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  
  d[[x]] <- get_question(questionid = x, good_manners = 0)
  #paste0("stortingsporsmal", x) = rbind(a, b, c)
}


questiontext <- do.call("rbind", d)

save(questionlist, file = "Question_Data/MetadataQuestionList.Rdata")
save(questiontext, file = "Question_Data/All_Questions.Rdata")

```

This now gives us a dataframe for analysis. After this we approach the data from two ways, we create a structural topic model, and we use this to analyse the totality of the questions dataframe. 

The structural topic model is created with the code below. First we have to include the answers as well, as part of the documents. 

```{r, eval=FALSE}
answerlist <- questiontext %>%
  select(id, text = answer_text, title, type, question_from_id, 
         qustion_to_id)

## Dropping the observations with no text, note that no text in 
## Stortingscrape is coded as empty string, not as an NA

answerlist <- answerlist %>%
  mutate(notapplicable = ifelse(text == "", NA, 1)) %>%
  drop_na(notapplicable) %>%
  select(-notapplicable)

## Limiting the dataset

questionshort <- questiontext %>%
  select(id, text = question_text, title, type, question_from_id,
         qustion_to_id)

## Binding the sets together, so I have one dataframe with complete
## texts from either answers or questions. **OF NOTE** I should probably
## have added a string to the id's so that i separate answer texts 
## from question texts, given their identical id.

assembledtextquestions <- rbind(questionshort, answerlist)

## removing reduntant objects

rm(answerlist, questionlist, questionshort, questiontext)

## First i turn all strings into lower case

assembledtextquestions <- assembledtextquestions %>% 
  mutate(text_l = str_to_lower(text))

## Before tokenizing I create a stopword dataset, I add br, because it
## is not properly removed from some observations. 

stop_words <- get_stopwords(language = "no")

removeword <- data.frame(word = "br", lexicon = "own library")

stop_words <- rbind(stop_words, removeword)

## Next I tokennize the text, this takes a moment. 

questiontokens <- assembledtextquestions %>%
  unnest_tokens(input = text, # Which variable to get the text from
                output = word, # What the new variable should be called
                token = "words") # Type of tokens to split the text into

## Removing all stopwords from my stop words book

questiontokens <- questiontokens %>%
  anti_join(stop_words, by = "word") # Joining against the stopwords dataframe to get rid of cells with stopwords

## Brief category investigation, stock companies are frequent

questiontokens %>%
  count(id, word, sort = TRUE)

## Creating the document feature matrix

questiontokens <- questiontokens %>%
  mutate(stem = wordStem(word, language = "norwegian"))

questiontokens_dfm <- questiontokens %>%
  count(id, stem, name = "count") %>% # By default, the count-variable is called "n". Use name = "" to change it.
  cast_dfm(id, # Specify the douments in your analysis (becoming the rows)
           stem, # Specify the tokens in your analysis (becoming the columns)
           count) # Specify the number of times each token shows up in each document (becoming the cells)

## Saving these objects, ready for analysis, note that the tokens dataframe
## is far too large to be uploaded to GitHub

save(questiontokens_dfm, file = "Question_Data/questionsdfm.Rdata")
save(questiontokens, file = "Question_Data/questiontokens.Rdata")

```

With the document feature matrix, we can apply an stm function to it. We run it with 75 categories, slightly more than what the package itself reccommends for a corpus of this size.

```{r, eval=FALSE}
questiontokens_lda_75 <- stm(questiontokens_dfm,
                             init.type = "LDA",
                             K = 75,
                             seed = 910,
                             verbose = TRUE, 
                             max.em.its = 100, ## You can adjust these numbers
                             emtol = 1e-5) ## However, too high numbers will drastically increase computing time


save(questiontokens_lda_75, file = "Question_Analysis/QuestionSTM_K75.Rdata")

## Save the object, because the model takes an eternity to run.

```

The model has now been run, and we have a Latent Dirichlect Allocation model with 75 topics and a corpus of 27 792 documents. This can be used to assign certain document topics, based on the models estimation of their Gamma value. 

The next step is then to make a grouping of the tokens, and creating a frame of all the documents and their gamma for each topic

```{r, eval=FALSE}
questiontopics_75 <- tidy(questiontokens_lda_75, 
                       matrix = "beta")

## Grouping by topic and getting the highest charging words for each topic

questiontopics_group_75 <- questiontopics_75 %>%
  group_by(topic) %>% # Getting the top term per topic, thus using group_by
  slice_max(beta, n = 10) %>% # Fetching the 10 terms with the highest beta
  ungroup() # Ungrouping to get the dataframe back to normal

### This is to create 

questiontopics_group_75_1 <- questiontopics_group_75 %>%
  filter(topic %in% c(1:8))

questiontopics_group_75_2 <- questiontopics_group_75 %>%
  filter(topic %in% c(9:15))

questiontopics_group_75_3 <- questiontopics_group_75 %>%
  filter(topic %in% c(16:23))

questiontopics_group_75_4 <- questiontopics_group_75 %>%
  filter(topic %in% c(24:30))

questiontopics_group_75_5 <- questiontopics_group_75 %>%
  filter(topic %in% c(31:38))

questiontopics_group_75_6 <- questiontopics_group_75 %>%
  filter(topic %in% c(39:45))

questiontopics_group_75_7 <- questiontopics_group_75 %>%
  filter(topic %in% c(46:53))

questiontopics_group_75_8 <- questiontopics_group_75 %>%
  filter(topic %in% c(54:60))

questiontopics_group_75_9 <- questiontopics_group_75 %>%
  filter(topic %in% c(61:68))

questiontopics_group_75_10 <- questiontopics_group_75 %>%
  filter(topic %in% c(68:75))

## These groupings can be used later, when we discover what topics become
## We can then use these groupings to sort out what the topics really are about

#### Making the matrix of Gammas for each document

question_doc_prob_75 <- tidy(questiontokens_lda_75, matrix = "gamma", # Calculating document probabilities
                    document_names = rownames(questiontokens_dfm)) # Adding the question id's

### Then we assign each document their top 3 topics 

top_docs <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  slice_max(gamma, n = 3) # Find the max value

topic_assignment <- top_docs %>%
  group_by(document) %>%
  summarise(TopTopic = first(topic),
            SecTopic = nth(topic, 2),
            ThirdTopic = nth(topic, 3),
            TopGamma = first(gamma),
            SecGamma = nth(gamma, 2), 
            ThirdGamma = nth(gamma, 3))

### Renaming document to id, because the name of the documents are used to match
### the full data later

topic_assignment <- topic_assignment %>%
  rename(id = document)

```

The next step is then to join these variables with our full set of questions. Leaving us with a dataset that we can filter out keywords on. The previous frames and objects will still be used later, when we explore alternative topic spreads. 

```{r, eval=FALSE}
load("Question_Data/All_Questions.Rdata")

## Changing it all to lower case.

questiontext <- questiontext %>%
  mutate(text_q_l = str_to_lower(question_text),
         text_a_l = str_to_lower(answer_text),
         justification_l = str_to_lower(justification))

## Filtering out our relevant texts

textrelevance <- questiontext %>%
  filter(str_detect(text_q_l, "fn-sambandet") |
           str_detect(text_q_l, "utdanning for bærekraftig utvikling") |
           str_detect(text_q_l, "utdanning for berekraftig utvikling") |
           str_detect(text_q_l, "unicef") | 
           str_detect(text_q_l, "wfp") | 
           str_detect(text_q_l, "unaids") |
           str_detect(text_a_l, "fn-sambandet") |
           str_detect(text_a_l, "utdanning for bærekraftig utvikling") |
           str_detect(text_a_l, "utdanning for berekraftig utvikling") |
           str_detect(text_a_l, "unicef") | 
           str_detect(text_a_l, "wfp") | 
           str_detect(text_a_l, "unaids") |
           str_detect(justification_l, "fn-sambandet") |
           str_detect(justification_l, "utdanning for bærekraftig utvikling") |
           str_detect(justification_l, "utdanning for berekraftig utvikling") |
           str_detect(justification_l, "unicef") | 
           str_detect(justification_l, "wfp") | 
           str_detect(justification_l, "unaids") |
           str_detect(text_q_l, "unesco") |
           str_detect(text_a_l, "unesco") |
           str_detect(justification_l, "unesco") |
           str_detect(text_q_l, "undp") |
           str_detect(text_a_l, "undp") |
           str_detect(justification_l, "undp") |
           str_detect(text_q_l, "\\b(ilo)\\b") |
           str_detect(text_a_l, "\\b(ilo)\\b") |
           str_detect(justification_l, "\\b(ilo)\\b") |
           str_detect(text_q_l, "fns matvareprogram") |
           str_detect(text_a_l, "fns matvareprogram") |
           str_detect(justification_l, "fns matvareprogram") |
           str_detect(text_q_l, "who") |
           str_detect(text_a_l, "who") |
           str_detect(justification_l, "who") |
           str_detect(text_q_l, "verdens helseorganisasjon") |
           str_detect(text_a_l, "verdens helseorganisasjon") |
           str_detect(justification_l, "verdens helseorganisasjon") |
           str_detect(text_q_l, "ipcc") |
           str_detect(text_a_l, "ipcc") |
           str_detect(justification_l, "ipcc") |
           str_detect(text_q_l, "fns klimapanel") |
           str_detect(text_a_l, "fns klimapanel") |
           str_detect(justification_l, "fns klimapanel") |
           str_detect(text_q_l, "bærekraftsmål") |
           str_detect(text_a_l, "bærekraftsmål") |
           str_detect(justification_l, "bærekraftsmål") |
           str_detect(text_q_l, "berekraftsmål") |
           str_detect(text_a_l, "berekraftsmål") |
           str_detect(justification_l, "berekraftsmål") |
           str_detect(text_q_l, "2030-agendaen") |
           str_detect(text_a_l, "2030-agendaen") |
           str_detect(justification_l, "2030-agendaen") |
           str_detect(text_q_l, "agenda 2030") |
           str_detect(text_a_l, "agenda 2030") |
           str_detect(justification_l, "agenda 2030"))

## Joining them with the top 3 topics for their document. 

textrelevance <- left_join(textrelevance, topic_assignment, by = "id")

## lets have a look at the most frequent topics

ggplot(textrelevance, aes(x = TopTopic)) + 
  geom_histogram(binwidth = 1)

## Most common top topic is topic 54,

ggplot(textrelevance, aes(x = SecTopic)) + 
  geom_histogram(binwidth = 1)

ggplot(textrelevance, aes(x = ThirdTopic)) + 
  geom_histogram(binwidth = 1)

```