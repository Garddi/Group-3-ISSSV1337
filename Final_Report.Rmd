---
title: "ISSSV-1337, Group 3 Report UN Association of Norway"
author: "Gard, Markus, Andreas, Tia, Nora, Ingrid & Hannes"
date: '2022-07-25'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Given Objective

The task provided to us, was to provide the UNA with "quantitative measures" of their work. The wish from our mission provider was to gain knowledge about how much the members of parliament are aware of the activities of the UN


## Problem 

The central problem at hand is how to measure the impact of lobbying, as direct contact between politicians is difficult to find and to measure. 


## Our Approach

In order to map the knowledge of the activities of the UNA, we decided to collect all available texts from Stortinget. After gathering these texts, we needed to figure out how to measure relative prevalence in our data. To this we first decided to create a list over relevant key terms that represent the work of the UNA. We divided these into three levels, based on the suggestions provided to us by our mission provider. The top level search word is "FN-sambandet" the UNA in Norwegian. The first thing that struck us was the fact that this yielded very few results, and as such we saw it necessary to focus much of our work outside what UNA has been explicitly mentioned in. 

Our mission description also contained more suggestions for keywords, and so we used those to create level 2 and level 3 searchwords. Level 2 consists of a particular goal that our provider explained that they had been working towards. This goal was UN sustainability goal 4.7 Education for Sustainable Development. The keywords here were also used to investigate, but again returned very few results. The problems of this will be elaborated on in a later section. 

The final level of search words was the relevant humanitarian sub organisations of UN, such as UNAIDS, WFP, UNDP and UNICEF. These ended up forming the backbone of our approach, as they were far more prevalent, however, all words were maintained in the end, when a complete analysis was performed. 

Finally it is worth mentioning that we also foresaw the large amount of documents being being problematic, as such, we early on decided to limit our timeframe. The package stortingsscrape informs us that voting data is only available from the 2011-2012 session and onwards, so we viewed that as a natural starting point. 

# Joint setup code and used packages

Below we include a setup code for the things we do in this script, this primarily includes the relevant packages, but also included is a function for a colourblind safe palette, given that we are interested in accessibility. 

```{r,eval=TRUE, message=FALSE}

library(tidyverse)
library(stortingscrape)
library(tidytext)
library(quanteda)
library(stm)
library(tidyr)
library(viridis)
library(SnowballC)
library(haven)
library(rvest)
library(xml2)
library(httr)
library(reshape)


safe_colorblind_palette <- c("#88CCEE", "#CC6677", "#DDCC77", 
                             "#117733", "#332288", "#AA4499", 
                             "#44AA99", "#999933", "#882255", 
                             "#661100", "#6699CC", "#888888")


Sys.setlocale(category = "LC_ALL", "")

```
## Sectioning 

Due to the large corpus that would arise from joining all documents, and the severe lack of coherent genre textuality, and to the unique opportunities afforded by each form of retrievable information, we decided to split up our work, and pursue separate roads of inquiry based on each section. 

# Questions 

The questions were scraped, below is the plot drawing the overview of the spreading of mentions of our relevant keywords over the relevant timeframe.

## The questions we ask of the questions

 - Who are asking questions about the UN and the UNA?
 - What are these questions about?
 - What questions mentions the UNA and the associated UN organizations?
 - How are these talked about?
 - Do these questions touch on issues regarding sustainability and other topics of interest that the UNA might be interested in?
 - Other interesting patterns in the questions
 
In order to answer this, we scrape all the questions from the storting's homepage. The following code scrapes all the questions for our relevant time frame. 

```{r, eval=TRUE}

## For now the sessions we are interested in are limited to 2011-2022

sessions_storting <- get_parlsessions()

sessions_storting <- sessions_storting %>%
  filter(id %in% c("2011-2012", "2012-2013", "2013-2014",
                   "2014-2015", "2015-2016", "2016-2017",
                   "2017-2018", "2018-2019", "2019-2020", 
                   "2020-2021", "2021-2022"))
```

This section retrieves all the sessions that we are interested in, the object `sessions_storting` will be used for most of our code chunks later.

The next step is then to retrieve the metadata on all the questions.

```{r, eval=FALSE}
## The id's are then entered into this loop, which *should* retrieve 
## ALL question id's for our relevant period. 

a<-list()
b<-list()
c<-list()

for(x in unique(sessions_storting$id)){
  a[[x]] <- get_session_questions(sessionid = x, 
                                  q_type = "interpellasjoner", 
                                  status = NA, good_manners = 0)
  b[[x]] <- get_session_questions(sessionid = x, 
                                  q_type = "sporretimesporsmal",
                                  status = NA, good_manners = 0)
  c[[x]] <- get_session_questions(sessionid = x, 
                                  q_type = "skriftligesporsmal", 
                                  status = NA, good_manners = 0)
}

## Unlisting all of the resulting lists, they are lists of identically 
## sized dataframes.

questionlista <- do.call("rbind", a)
questionlistb <- do.call("rbind", b)
questionlistc <- do.call("rbind", c)

clist <- list(questionlista, questionlistb, questionlistc)

## Combining all for a dataframe we can fetch 

questionlist <- do.call("rbind", clist)

```

This has created our full list of questions, but not the text of the question and answer themselves. The next step is retrieving all of those. 

```{r, eval = FALSE}

d <- list()

for(x in unique(questionlist$id)){
  it <- 100*(which(unique(questionlist$id) == x) / length(unique(questionlist$id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  
  d[[x]] <- get_question(questionid = x, good_manners = 0)
  #paste0("stortingsporsmal", x) = rbind(a, b, c)
}


questiontext <- do.call("rbind", d)

save(questionlist, file = "Question_Data/MetadataQuestionList.Rdata")
save(questiontext, file = "Question_Data/All_Questions.Rdata")

```

This now gives us a dataframe for analysis. After this we approach the data from two ways, we create a structural topic model, and we use this to analyse the totality of the questions dataframe. 

The structural topic model is created with the code below. First we have to include the answers as well, as part of the documents. 

```{r, eval=FALSE}
answerlist <- questiontext %>%
  select(id, text = answer_text, title, type, question_from_id, 
         qustion_to_id)

## Dropping the observations with no text, note that no text in 
## Stortingscrape is coded as empty string, not as an NA

answerlist <- answerlist %>%
  mutate(notapplicable = ifelse(text == "", NA, 1)) %>%
  drop_na(notapplicable) %>%
  select(-notapplicable)

## Limiting the dataset

questionshort <- questiontext %>%
  select(id, text = question_text, title, type, question_from_id,
         qustion_to_id)

## Binding the sets together, so I have one dataframe with complete
## texts from either answers or questions. **OF NOTE** I should probably
## have added a string to the id's so that i separate answer texts 
## from question texts, given their identical id.

assembledtextquestions <- rbind(questionshort, answerlist)

## removing reduntant objects

rm(answerlist, questionlist, questionshort, questiontext)

## First i turn all strings into lower case

assembledtextquestions <- assembledtextquestions %>% 
  mutate(text_l = str_to_lower(text))

## Before tokenizing I create a stopword dataset, I add br, because it
## is not properly removed from some observations. 

stop_words <- get_stopwords(language = "no")

removeword <- data.frame(word = "br", lexicon = "own library")

stop_words <- rbind(stop_words, removeword)

## Next I tokennize the text, this takes a moment. 

questiontokens <- assembledtextquestions %>%
  unnest_tokens(input = text, # Which variable to get the text from
                output = word, # What the new variable should be called
                token = "words") # Type of tokens to split the text into

## Removing all stopwords from my stop words book

# Joining against the stopwords dataframe to get rid of cells with stopwords
questiontokens <- questiontokens %>%
  anti_join(stop_words, by = "word") 

## Brief category investigation, stock companies are frequent

questiontokens %>%
  count(id, word, sort = TRUE)

## Creating the document feature matrix

questiontokens <- questiontokens %>%
  mutate(stem = wordStem(word, language = "norwegian"))

questiontokens_dfm <- questiontokens %>%
  count(id, stem, name = "count") %>% 
  cast_dfm(id, 
           stem, 
           count) 

## Saving these objects, ready for analysis, note that the tokens dataframe
## is far too large to be uploaded to GitHub

save(questiontokens_dfm, file = "Question_Data/questionsdfm.Rdata")
save(questiontokens, file = "Question_Data/questiontokens.Rdata")

```

With the document feature matrix, we can apply an stm function to it. We run it with 75 categories, slightly more than what the package itself reccommends for a corpus of this size.

```{r, eval=FALSE}
questiontokens_lda_75 <- stm(questiontokens_dfm,
                             init.type = "LDA",
                             K = 75,
                             seed = 910,
                             verbose = TRUE, 
                             max.em.its = 100, # You can adjust these numbers
                             emtol = 1e-5) # However, too high numbers will 
                                          #drastically increase computing time


save(questiontokens_lda_75, file = "Question_Analysis/QuestionSTM_K75.Rdata")

## Save the object, because the model takes an eternity to run.

```

The model has now been run, and we have a Latent Dirichlect Allocation model with 75 topics and a corpus of 27 792 documents. This can be used to assign certain document topics, based on the models estimation of their Gamma value. 

The next step is then to make a grouping of the tokens, and creating a frame of all the documents and their gamma for each topic

```{r, eval=TRUE}

load("Question_Analysis/QuestionSTM_K75.Rdata")
load("Question_Data/questionsdfm.Rdata")

questiontopics_75 <- tidy(questiontokens_lda_75, 
                       matrix = "beta")

## Grouping by topic and getting the highest charging words for each topic

questiontopics_group_75 <- questiontopics_75 %>%
  group_by(topic) %>% # Getting the top term per topic, thus using group_by
  slice_max(beta, n = 10) %>% # Fetching the 10 terms with the highest beta
  ungroup() # Ungrouping to get the dataframe back to normal

## Quite difficult to plot all of them at the same time, so that is not included
## here, but is has been done in order to find some interesting topics. 


#### Making the matrix of Gammas for each document

question_doc_prob_75 <- tidy(questiontokens_lda_75, matrix = "gamma", 
                    document_names = rownames(questiontokens_dfm)) 

### Then we assign each document their top 3 topics 

top_docs <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  slice_max(gamma, n = 3) # Find the max value

topic_assignment <- top_docs %>%
  group_by(document) %>%
  summarise(TopTopic = first(topic),
            SecTopic = nth(topic, 2),
            ThirdTopic = nth(topic, 3),
            TopGamma = first(gamma),
            SecGamma = nth(gamma, 2), 
            ThirdGamma = nth(gamma, 3))

### Renaming document to id, because the name of the documents are used to match
### the full data later

topic_assignment <- topic_assignment %>%
  dplyr::rename(id = document)

```

The next step is then to join these variables with our full set of questions. Leaving us with a dataset that we can filter out keywords on. The previous frames and objects will still be used later, when we explore alternative topic spreads. 

```{r, eval=TRUE}
load("Question_Data/All_Questions.Rdata")

## Changing it all to lower case.

questiontext <- questiontext %>%
  mutate(text_q_l = str_to_lower(question_text),
         text_a_l = str_to_lower(answer_text),
         justification_l = str_to_lower(justification))

## Filtering out our relevant texts

textrelevance <- questiontext %>%
  filter(str_detect(text_q_l, "fn-sambandet") |
           str_detect(text_q_l, "utdanning for bærekraftig utvikling") |
           str_detect(text_q_l, "utdanning for berekraftig utvikling") |
           str_detect(text_q_l, "unicef") | 
           str_detect(text_q_l, "wfp") | 
           str_detect(text_q_l, "unaids") |
           str_detect(text_a_l, "fn-sambandet") |
           str_detect(text_a_l, "utdanning for bærekraftig utvikling") |
           str_detect(text_a_l, "utdanning for berekraftig utvikling") |
           str_detect(text_a_l, "unicef") | 
           str_detect(text_a_l, "wfp") | 
           str_detect(text_a_l, "unaids") |
           str_detect(justification_l, "fn-sambandet") |
           str_detect(justification_l, "utdanning for bærekraftig utvikling") |
           str_detect(justification_l, "utdanning for berekraftig utvikling") |
           str_detect(justification_l, "unicef") | 
           str_detect(justification_l, "wfp") | 
           str_detect(justification_l, "unaids") |
           str_detect(text_q_l, "unesco") |
           str_detect(text_a_l, "unesco") |
           str_detect(justification_l, "unesco") |
           str_detect(text_q_l, "undp") |
           str_detect(text_a_l, "undp") |
           str_detect(justification_l, "undp") |
           str_detect(text_q_l, "\\b(ilo)\\b") |
           str_detect(text_a_l, "\\b(ilo)\\b") |
           str_detect(justification_l, "\\b(ilo)\\b") |
           str_detect(text_q_l, "fns matvareprogram") |
           str_detect(text_a_l, "fns matvareprogram") |
           str_detect(justification_l, "fns matvareprogram") |
           str_detect(text_q_l, "who") |
           str_detect(text_a_l, "who") |
           str_detect(justification_l, "who") |
           str_detect(text_q_l, "verdens helseorganisasjon") |
           str_detect(text_a_l, "verdens helseorganisasjon") |
           str_detect(justification_l, "verdens helseorganisasjon") |
           str_detect(text_q_l, "ipcc") |
           str_detect(text_a_l, "ipcc") |
           str_detect(justification_l, "ipcc") |
           str_detect(text_q_l, "fns klimapanel") |
           str_detect(text_a_l, "fns klimapanel") |
           str_detect(justification_l, "fns klimapanel") |
           str_detect(text_q_l, "bærekraftsmål") |
           str_detect(text_a_l, "bærekraftsmål") |
           str_detect(justification_l, "bærekraftsmål") |
           str_detect(text_q_l, "berekraftsmål") |
           str_detect(text_a_l, "berekraftsmål") |
           str_detect(justification_l, "berekraftsmål") |
           str_detect(text_q_l, "2030-agendaen") |
           str_detect(text_a_l, "2030-agendaen") |
           str_detect(justification_l, "2030-agendaen") |
           str_detect(text_q_l, "agenda 2030") |
           str_detect(text_a_l, "agenda 2030") |
           str_detect(justification_l, "agenda 2030"))

## Joining them with the top 3 topics for their document. 

textrelevance <- left_join(textrelevance, topic_assignment, by = "id")

## lets have a look at the most frequent topics

toppingtopics <- textrelevance %>% 
  group_by(TopTopic) %>% 
  add_count() %>% 
  summarise(count = mean(n)) %>% 
  slice_max(order_by = count, n = 10)

toppingtopicswords <- questiontopics_group_75 %>% 
  filter(topic %in% toppingtopics$TopTopic)

## Lets look at what these topics are about

toppingtopicswords %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_bar(stat = "identity") + 
  facet_wrap( ~ topic, 
              ncol = 3, 
              scales = "free") + 
  labs(x = "", y = "Word-Topic probability") + 
  theme_bw() + 
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

```

Shocking, I know, documents that mention UN organizations are usually about the UN. The interesting thing is that UN organizations are frequently used in talks regarding climate policy and emissions policies. This implies that this sector is more ahead relatively speaking in regards to having a global view on the problems at hand. This is perhaps unsurprising given the global nature of the issue, but perhaps an important path in the future can be to focus on making sure that some other topics, related to global issues are also related more to UN and their organizations. 

We should probably investigate the secondary topics, especially since topic 73 is a dedicated nynorsk category. It helps to look at the secondary categories. 


```{r, eval=TRUE}
secondtopics <- textrelevance %>% 
  group_by(SecTopic) %>% 
  add_count() %>% 
  summarise(count = mean(n)) %>% 
  slice_max(order_by = count, n = 10)

secondtopicswords <- questiontopics_group_75 %>% 
  filter(topic %in% secondtopics$SecTopic)

## Lets look at what these topics are about

secondtopicswords %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_bar(stat = "identity") + 
  facet_wrap( ~ topic, 
              ncol = 3, 
              scales = "free") + 
  labs(x = "", y = "Word-Topic probability") + 
  theme_bw() + 
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


```{r,eval=TRUE}
thirdtopics <- textrelevance %>% 
  group_by(ThirdTopic) %>% 
  add_count() %>% 
  summarise(count = mean(n)) %>% 
  slice_max(order_by = count, n = 10)

thirdtopicswords <- questiontopics_group_75 %>% 
  filter(topic %in% thirdtopics$ThirdTopic)

## Lets look at what these topics are about

thirdtopicswords %>%
  ggplot(aes(term, beta, fill = topic)) + 
  geom_bar(stat = "identity") + 
  facet_wrap( ~ topic, 
              ncol = 3, 
              scales = "free") + 
  labs(x = "", y = "Word-Topic probability") + 
  theme_bw() + 
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Topic 55 charges quite highly in this third topic category. This appears to be a generic "plan of action" category, most likely a common topic for all questions which asks for coherent national policy in fields. The problem here is that we are currently scraping quite low gamma values, so more common topics are brought to the forefront.

So we have talked about what the questions that mention the UN and their associated organisations are talking about, what these questions are substantially about. Unsurprisingly these questions are mostly about climate, health, and the UN itself. Lets now look at some general spread of the questions, when are these questions asked? Do we see a historic pattern?

First lets add some variables of interest, such as mp identities and join it with out priority level. 

```{r,eval=TRUE}

## First we add the priority levels for some of our observations. 

textrelevance <- textrelevance %>%
  mutate(priority_word_level = case_when(str_detect(text_q_l, "fn-sambandet") ~ "1",
           str_detect(text_q_l, "utdanning for bærekraftig utvikling")  ~ "2", 
           str_detect(text_q_l, "utdanning for berekraftig utvikling") ~ "2",
           str_detect(text_q_l, "unicef") ~ "3",
           str_detect(text_q_l, "wfp") ~ "3",
           str_detect(text_q_l, "unaids") ~ "3",
           str_detect(text_a_l, "fn-sambandet") ~ "1",
           str_detect(text_a_l, "utdanning for bærekraftig utvikling") ~ "2",
           str_detect(text_a_l, "utdanning for berekraftig utvikling") ~ "2",
           str_detect(text_a_l, "unicef") ~ "3",
           str_detect(text_a_l, "wfp") ~ "3",
           str_detect(text_a_l, "unaids") ~ "3",
           str_detect(justification_l, "fn-sambandet") ~ "1",
           str_detect(justification_l, "utdanning for bærekraftig utvikling") ~ "2",
           str_detect(justification_l, "utdanning for berekraftig utvikling") ~ "2",
           str_detect(justification_l, "unicef") ~ "3",
           str_detect(justification_l, "wfp") ~ "3",
           str_detect(justification_l, "unaids") ~ "3",
           str_detect(text_q_l, "unesco") ~ "3",
           str_detect(text_a_l, "unesco") ~ "3",
           str_detect(justification_l, "unesco") ~ "3",
           str_detect(text_q_l, "undp") ~ "3",
           str_detect(text_a_l, "undp") ~ "3",
           str_detect(justification_l, "undp") ~ "3",
           str_detect(text_q_l, "\\b(ilo)\\b") ~ "3",
           str_detect(text_a_l, "\\b(ilo)\\b") ~ "3",
           str_detect(justification_l, "\\b(ilo)\\b") ~ "3",
           str_detect(text_q_l, "fns matvareprogram") ~ "3",
           str_detect(text_a_l, "fns matvareprogram") ~ "3",
           str_detect(justification_l, "fns matvareprogram") ~ "3",
           str_detect(text_q_l, "who") ~ "3",
           str_detect(text_a_l, "who") ~ "3",
           str_detect(justification_l, "who") ~ "3",
           str_detect(text_q_l, "verdens helseorganisasjon") ~ "3",
           str_detect(text_a_l, "verdens helseorganisasjon") ~ "3",
           str_detect(justification_l, "verdens helseorganisasjon") ~ "3",
           str_detect(text_q_l, "ipcc") ~ "3", 
           str_detect(text_a_l, "ipcc") ~ "3",
           str_detect(justification_l, "ipcc") ~ "3",
           str_detect(text_q_l, "fns klimapanel") ~ "3",
           str_detect(text_a_l, "fns klimapanel") ~ "3",
           str_detect(justification_l, "fns klimapanel") ~ "3",
           str_detect(text_q_l, "bærekraftsmål") ~ "3",
           str_detect(text_a_l, "bærekraftsmål") ~ "3",
           str_detect(justification_l, "bærekraftsmål") ~ "3",
           str_detect(text_q_l, "berekraftsmål") ~ "3",
           str_detect(text_a_l, "berekraftsmål") ~ "3",
           str_detect(justification_l, "berekraftsmål") ~ "3",
           str_detect(text_q_l, "2030-agendaen") ~ "3",
           str_detect(text_a_l, "2030-agendaen") ~ "3",
           str_detect(justification_l, "2030-agendaen") ~ "3",
           str_detect(text_q_l, "agenda 2030") ~ "3",
           str_detect(text_a_l, "agenda 2030") ~ "3",
           str_detect(justification_l, "agenda 2030") ~ "3",
           str_detect(text_q_l, "bærekraftsmål 4") ~ "2",
           str_detect(text_a_l, "bærekraftsmål 4") ~ "2",
           str_detect(justification_l, "bærekraftsmål 4") ~ "2"))

## First we need to get all the mps for the period, then join them with the full 
## dataset

periods_storting <- get_parlperiods()

periods_storting <- periods_storting %>%
  filter(id %in% c("2021-2025", "2017-2021", "2013-2017", "2009-2013"))

j <- list()

for (x in periods_storting$id) {
  j[[x]] <- get_parlperiod_mps(periodid = x, substitute = TRUE, 
                               good_manners = 0)
}


allmps <- do.call("rbind", j)

allmpsparties <- allmps %>% 
  select(question_from_id = mp_id, gender, party_id, county_id, period_id)

textrelevance <- textrelevance %>%
  mutate(period_id = case_when(session_id == "2021-2022" ~ "2021-2025",
                               session_id == "2020-2021" ~ "2017-2021",
                               session_id == "2019-2020" ~ "2017-2021",
                               session_id == "2018-2019" ~ "2017-2021",
                               session_id == "2017-2018" ~ "2017-2021",
                               session_id == "2016-2017" ~ "2013-2017",
                               session_id == "2015-2016" ~ "2013-2017",
                               session_id == "2014-2015" ~ "2013-2017",
                               session_id == "2013-2014" ~ "2013-2017",
                               session_id == "2012-2013" ~ "2009-2013",
                               session_id == "2011-2012" ~ "2009-2013"))

textrelevancewparty <- left_join(textrelevance, allmpsparties, 
                                 by = c("question_from_id", "period_id"))

## Finally we assign each question as comming from either a government party,
## an opposition party, or a confidence and supply party

textrelevancewparty <- textrelevancewparty %>% 
  mutate(govposition = case_when(party_id == "A" & session_id == "2021-2022" ~ "Government",
                    party_id == "Sp" & session_id == "2021-2022" ~ "Government",
                    party_id == "SV" & session_id == "2021-2022" ~ "Confidence and Supply",
                    party_id == "H" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "FrP" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "KrF" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "MDG" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "R" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "V" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "A" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "Sp" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "SV" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "H" & session_id == "2020-2021" ~ "Government", 
                    party_id == "FrP" & session_id == "2020-2021" ~ "Confidence and Supply",
                    party_id == "KrF" & session_id == "2020-2021" ~ "Government",
                    party_id == "MDG" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "R" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "V" & session_id == "2020-2021" ~ "Government",
                    party_id == "A" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "Sp" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "SV" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "H" & session_id == "2019-2020" ~ "Government", 
                    party_id == "FrP" & session_id == "2019-2020" ~ "Confidence and Supply",
                    party_id == "KrF" & session_id == "2019-2020" ~ "Government",
                    party_id == "MDG" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "R" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "V" & session_id == "2019-2020" ~ "Government",
                    party_id == "A" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "Sp" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "SV" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "H" & session_id == "2018-2019" ~ "Government", 
                    party_id == "FrP" & session_id == "2018-2019" ~ "Government",
                    party_id == "KrF" & session_id == "2018-2019" ~ "Government",
                    party_id == "MDG" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "R" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "V" & session_id == "2018-2019" ~ "Government",
                    party_id == "A" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "Sp" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "SV" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "H" & session_id == "2017-2018" ~ "Government", 
                    party_id == "FrP" & session_id == "2017-2018" ~ "Government",
                    party_id == "KrF" & session_id == "2017-2018" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "R" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "V" & session_id == "2017-2018" ~ "Government",
                    party_id == "A" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "Sp" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "SV" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "H" & session_id == "2016-2017" ~ "Government", 
                    party_id == "FrP" & session_id == "2016-2017" ~ "Government",
                    party_id == "KrF" & session_id == "2016-2017" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "R" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "V" & session_id == "2016-2017" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "Sp" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "SV" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "H" & session_id == "2015-2016" ~ "Government", 
                    party_id == "FrP" & session_id == "2015-2016" ~ "Government",
                    party_id == "KrF" & session_id == "2015-2016" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "R" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "V" & session_id == "2015-2016" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "Sp" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "SV" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "H" & session_id == "2014-2015" ~ "Government", 
                    party_id == "FrP" & session_id == "2014-2015" ~ "Government",
                    party_id == "KrF" & session_id == "2014-2015" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "R" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "V" & session_id == "2014-2015" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "Sp" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "SV" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "H" & session_id == "2013-2014" ~ "Government", 
                    party_id == "FrP" & session_id == "2013-2014" ~ "Government",
                    party_id == "KrF" & session_id == "2013-2014" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "R" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "V" & session_id == "2013-2014" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2012-2013" ~ "Government",
                    party_id == "Sp" & session_id == "2012-2013" ~ "Government",
                    party_id == "SV" & session_id == "2012-2013" ~ "Government",
                    party_id == "H" & session_id == "2012-2013" ~ "Opposition", 
                    party_id == "FrP" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "KrF" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "MDG" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "R" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "V" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "A" & session_id == "2011-2012" ~ "Government",
                    party_id == "Sp" & session_id == "2011-2012" ~ "Government",
                    party_id == "SV" & session_id == "2011-2012" ~ "Government",
                    party_id == "H" & session_id == "2011-2012" ~ "Opposition", 
                    party_id == "FrP" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "KrF" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "MDG" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "R" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "V" & session_id == "2011-2012" ~ "Opposition"
))

```

Now that we have a dataset with more info, lets look at some of the patterns we can notice over time.

```{r, eval=TRUE}
ggplot(textrelevancewparty, aes(x = session_id, fill = priority_word_level)) + 
  geom_bar() + 
  scale_fill_discrete(type = safe_colorblind_palette) + 
  theme_bw()

ggplot(textrelevancewparty, aes(x = session_id, fill = party_id)) + 
  geom_bar() + 
  scale_fill_discrete(type = safe_colorblind_palette) + 
  theme_bw()

ggplot(textrelevancewparty, aes(x = session_id, fill = gender)) + 
  geom_bar() + 
  scale_fill_viridis(discrete = TRUE) + 
  theme_bw()

ggplot(textrelevancewparty, aes(x = session_id, fill = govposition)) + 
  geom_bar() + 
  scale_fill_viridis(discrete = TRUE) + 
  theme_bw()
```

We see a clear spike in mentions in 2021-2022, likely due to the war in Ukraine. In order to further evaluate this patterns, we need to create a comparison to all the questions being asked, that is plotted below. 

```{r, eval=TRUE}

## Creating the same variables in the full dataframe (this is in fact a very 
## overcomplicated way of doing this, but it was done this way and now its legacy 
## code)

questiontext2 <- questiontext %>%
  mutate(period_id = case_when(session_id == "2021-2022" ~ "2021-2025",
                               session_id == "2020-2021" ~ "2017-2021",
                               session_id == "2019-2020" ~ "2017-2021",
                               session_id == "2018-2019" ~ "2017-2021",
                               session_id == "2017-2018" ~ "2017-2021",
                               session_id == "2016-2017" ~ "2013-2017",
                               session_id == "2015-2016" ~ "2013-2017",
                               session_id == "2014-2015" ~ "2013-2017",
                               session_id == "2013-2014" ~ "2013-2017",
                               session_id == "2012-2013" ~ "2009-2013",
                               session_id == "2011-2012" ~ "2009-2013"))

questiontextwparty <- left_join(questiontext2, allmpsparties, 
                                 by = c("question_from_id", "period_id"))


questiontextwparty <- questiontextwparty %>% 
  mutate(govposition = case_when(party_id == "A" & session_id == "2021-2022" ~ "Government",
                    party_id == "Sp" & session_id == "2021-2022" ~ "Government",
                    party_id == "SV" & session_id == "2021-2022" ~ "Confidence and Supply",
                    party_id == "H" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "FrP" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "KrF" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "MDG" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "R" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "V" & session_id == "2021-2022" ~ "Opposition",
                    party_id == "A" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "Sp" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "SV" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "H" & session_id == "2020-2021" ~ "Government", 
                    party_id == "FrP" & session_id == "2020-2021" ~ "Confidence and Supply",
                    party_id == "KrF" & session_id == "2020-2021" ~ "Government",
                    party_id == "MDG" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "R" & session_id == "2020-2021" ~ "Opposition",
                    party_id == "V" & session_id == "2020-2021" ~ "Government",
                    party_id == "A" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "Sp" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "SV" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "H" & session_id == "2019-2020" ~ "Government", 
                    party_id == "FrP" & session_id == "2019-2020" ~ "Confidence and Supply",
                    party_id == "KrF" & session_id == "2019-2020" ~ "Government",
                    party_id == "MDG" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "R" & session_id == "2019-2020" ~ "Opposition",
                    party_id == "V" & session_id == "2019-2020" ~ "Government",
                    party_id == "A" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "Sp" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "SV" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "H" & session_id == "2018-2019" ~ "Government", 
                    party_id == "FrP" & session_id == "2018-2019" ~ "Government",
                    party_id == "KrF" & session_id == "2018-2019" ~ "Government",
                    party_id == "MDG" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "R" & session_id == "2018-2019" ~ "Opposition",
                    party_id == "V" & session_id == "2018-2019" ~ "Government",
                    party_id == "A" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "Sp" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "SV" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "H" & session_id == "2017-2018" ~ "Government", 
                    party_id == "FrP" & session_id == "2017-2018" ~ "Government",
                    party_id == "KrF" & session_id == "2017-2018" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "R" & session_id == "2017-2018" ~ "Opposition",
                    party_id == "V" & session_id == "2017-2018" ~ "Government",
                    party_id == "A" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "Sp" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "SV" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "H" & session_id == "2016-2017" ~ "Government", 
                    party_id == "FrP" & session_id == "2016-2017" ~ "Government",
                    party_id == "KrF" & session_id == "2016-2017" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "R" & session_id == "2016-2017" ~ "Opposition",
                    party_id == "V" & session_id == "2016-2017" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "Sp" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "SV" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "H" & session_id == "2015-2016" ~ "Government", 
                    party_id == "FrP" & session_id == "2015-2016" ~ "Government",
                    party_id == "KrF" & session_id == "2015-2016" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "R" & session_id == "2015-2016" ~ "Opposition",
                    party_id == "V" & session_id == "2015-2016" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "Sp" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "SV" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "H" & session_id == "2014-2015" ~ "Government", 
                    party_id == "FrP" & session_id == "2014-2015" ~ "Government",
                    party_id == "KrF" & session_id == "2014-2015" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "R" & session_id == "2014-2015" ~ "Opposition",
                    party_id == "V" & session_id == "2014-2015" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "Sp" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "SV" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "H" & session_id == "2013-2014" ~ "Government", 
                    party_id == "FrP" & session_id == "2013-2014" ~ "Government",
                    party_id == "KrF" & session_id == "2013-2014" ~ "Confidence and Supply",
                    party_id == "MDG" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "R" & session_id == "2013-2014" ~ "Opposition",
                    party_id == "V" & session_id == "2013-2014" ~ "Confidence and Supply",
                    party_id == "A" & session_id == "2012-2013" ~ "Government",
                    party_id == "Sp" & session_id == "2012-2013" ~ "Government",
                    party_id == "SV" & session_id == "2012-2013" ~ "Government",
                    party_id == "H" & session_id == "2012-2013" ~ "Opposition", 
                    party_id == "FrP" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "KrF" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "MDG" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "R" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "V" & session_id == "2012-2013" ~ "Opposition",
                    party_id == "A" & session_id == "2011-2012" ~ "Government",
                    party_id == "Sp" & session_id == "2011-2012" ~ "Government",
                    party_id == "SV" & session_id == "2011-2012" ~ "Government",
                    party_id == "H" & session_id == "2011-2012" ~ "Opposition", 
                    party_id == "FrP" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "KrF" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "MDG" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "R" & session_id == "2011-2012" ~ "Opposition",
                    party_id == "V" & session_id == "2011-2012" ~ "Opposition"
))

## Plotting this 

ggplot(questiontextwparty, aes(x = session_id, fill = govposition)) + 
  geom_bar() + 
  scale_fill_viridis(discrete = TRUE) + 
  theme_bw()

ggplot(questiontextwparty, aes(x = session_id, fill = gender)) + 
  geom_bar() + 
  scale_fill_viridis(discrete = TRUE) + 
  theme_bw()


ggplot(questiontextwparty, aes(x = session_id, fill = party_id)) + 
  geom_bar() + 
  scale_fill_discrete(type = safe_colorblind_palette) + 
  theme_bw()

```

In summary, if we compare these to the results of our keyword containing documents, we see similar patterns in terms of both gender and government position, this is perhaps unsurprising. We do however see some significant differences in terms of parties asking questions. The liberal party (V) and the Socialist Left (SV) are heavily overrepresented. In addition the Progress party (FrP), was much more active during the Stoltenberg 2 Government, than they are against the Støre government. Hinting at abandoning a focus on international aid, perhaps due to the leadership change in 2020. 

### What else can we use the topic model for?

Interestingly the topic 47 seems to be about humanitarian aid and the UN in general, we can use this to find the documents that are most likely to be about UN. 

```{r, eval=TRUE}
## The Gamma for these documents are in fact quite high. But more importantly 
## they touch on the UN and their activities in a manner unrelated to security
## policy. 

topic47docs <- question_doc_prob_75 %>% 
  filter(topic == 47) %>% 
  slice_max(order_by = gamma, n = 650) 

top100question <- questiontextwparty %>% 
  filter(id %in% topic47docs$document)

ggplot(top100question, aes(x = session_id, fill = party_id)) + 
  geom_bar() +
  scale_fill_discrete(type = safe_colorblind_palette) +
  theme_bw()

ggplot(top100question, aes(x = session_id, fill = govposition)) + 
  geom_bar() +
  scale_fill_discrete(type = safe_colorblind_palette) +
  theme_bw()

ggplot(top100question, aes(x = session_id, fill = gender)) + 
  geom_bar() + 
  scale_fill_discrete(type = safe_colorblind_palette) + 
  theme_bw()

```

Similar patterns as in the original keywords.

Lets now look at some other categories inside the topic model, and whether they are of interest. By looking closely at the different topics, we find that topics 30, 33, 43, 52, 54, and 63 are of interest. These topics primarily look at climate and conservation topics. However, topic 30 and 52 are interesting in particular. Topic 30 seems to be about making requirements of the minister that is being asked the question, attempting to hold them accountable. A correlation on this topic would imply that MPs are using the UN and their talking points to drive forwards critique of the government, holding them accountable. Topic 52 is also interesting because it is about higher education, and relating that to the UN and their suborganisations would be helpful to see if they connect it to the activities of global development. To explore these topics, we plot the density of the gammas of our top topic 47 documents, to see to what degree they actually measure this. 

```{r, eval=TRUE}

#Topic 30 is related to questions asked that require the minister in charge to do
# something. 

top_docs100topic30 <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  filter(topic == 30 & document %in% top100question$id)

ggplot(top_docs100topic30, aes(x = gamma)) + 
  geom_density() + 
  theme_bw() + 
  labs(title = "Density of Topic 30")

#Topic 33 is about emissions. 

top_docs100topic33 <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  filter(topic == 33 & document %in% top100question$id)

ggplot(top_docs100topic33, aes(x = gamma)) + 
  geom_density() + 
  theme_bw() + 
  labs(title = "Density of Topic 33")

#Topic 43 is about sustainability. 

top_docs100topic43 <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  filter(topic == 43 & document %in% top100question$id)

ggplot(top_docs100topic43, aes(x = gamma)) + 
  geom_density() + 
  theme_bw() + 
  labs(title = "Density of Topic 43")


#Topic 52, Higher education

top_docs100topic52 <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  filter(topic == 52 & document %in% top100question$id)

ggplot(top_docs100topic52, aes(x = gamma)) + 
  geom_density() + 
  theme_bw() + 
  labs(title = "Density of Topic 52")


#Topic 54, more climate emissions

top_docs100topic54 <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  filter(topic == 54 & document %in% top100question$id)

ggplot(top_docs100topic54, aes(x = gamma)) + 
  geom_density() + 
  theme_bw() + 
  labs(title = "Density of Topic 54")


#Topic 63, Environmentalism

top_docs100topic63 <- question_doc_prob_75 %>%
  group_by(document) %>% # Find the next statistic per document
  filter(topic == 63 & document %in% top100question$id)

ggplot(top_docs100topic63, aes(x = gamma)) + 
  geom_density() + 
  theme_bw() + 
  labs(title = "Density of Topic 63")

```

As a final point, since this way of looking at it doesnt account for mean values, I plot a heatmap of a correlation matrix of our topics, looking to see if there are any significant values that peak out in correlation.

```{r, eval=TRUE}
topic_assignment2 <- question_doc_prob_75 %>%
  group_by(document) %>%
  summarise(`1` = first(gamma),
            `2` = nth(gamma, 2),
            `3` = nth(gamma, 3),
            `4` = nth(gamma, 4),
            `5` = nth(gamma, 5),
            `6` = nth(gamma, 6),
            `7` = nth(gamma, 7),
            `8` = nth(gamma, 8),
            `9` = nth(gamma, 9),
            `10` = nth(gamma, 10),
            `11` = nth(gamma, 11),
            `12` = nth(gamma, 12),
            `13` = nth(gamma, 13),
            `14` = nth(gamma, 13),
            `15` = nth(gamma, 15),
            `16` = nth(gamma, 16),
            `17` = nth(gamma, 17),
            `18` = nth(gamma, 18),
            `19` = nth(gamma, 19),
            `20` = nth(gamma, 20),
            `21` = nth(gamma, 21),
            `22` = nth(gamma, 22),
            `23` = nth(gamma, 23),
            `24` = nth(gamma, 24),
            `25` = nth(gamma, 25),
            `26` = nth(gamma, 26),
            `27` = nth(gamma, 27),
            `28` = nth(gamma, 28),
            `29` = nth(gamma, 29),
            `30` = nth(gamma, 30),
            `31` = nth(gamma, 31),
            `32` = nth(gamma, 32),
            `33` = nth(gamma, 33),
            `34` = nth(gamma, 34),
            `35` = nth(gamma, 35),
            `36` = nth(gamma, 36),
            `37` = nth(gamma, 37),
            `38` = nth(gamma, 38),
            `39` = nth(gamma, 39),
            `40` = nth(gamma, 40),
            `41` = nth(gamma, 41),
            `42` = nth(gamma, 42),
            `43` = nth(gamma, 43),
            `44` = nth(gamma, 44),
            `45` = nth(gamma, 45),
            `46` = nth(gamma, 46),
            `47` = nth(gamma, 47),
            `48` = nth(gamma, 48),
            `49` = nth(gamma, 49),
            `50` = nth(gamma, 50),
            `51` = nth(gamma, 51),
            `52` = nth(gamma, 52),
            `53` = nth(gamma, 53),
            `54` = nth(gamma, 54),
            `55` = nth(gamma, 55),
            `56` = nth(gamma, 56),
            `57` = nth(gamma, 57),
            `58` = nth(gamma, 58),
            `59` = nth(gamma, 59),
            `60` = nth(gamma, 60),
            `61` = nth(gamma, 61),
            `62` = nth(gamma, 62),
            `63` = nth(gamma, 63),
            `64` = nth(gamma, 64),
            `65` = nth(gamma, 65),
            `66` = nth(gamma, 66),
            `67` = nth(gamma, 67),
            `68` = nth(gamma, 68),
            `69` = nth(gamma, 69),
            `70` = nth(gamma, 70),
            `71` = nth(gamma, 71),
            `72` = nth(gamma, 72),
            `73` = nth(gamma, 73),
            `74` = nth(gamma, 74),
            `75` = nth(gamma, 75))

corframe <- topic_assignment2 %>% 
  select(-(document))

rouncor <- round(cor(corframe), 2)

melted <- melt(rouncor)

ggplot(melted, aes(x = X1, y = X2, fill = value)) + 
  geom_tile() + 
  theme_bw() + 
  labs(title = "Correlation map between topic Gammas", x = "", 
       y = "")
```

Looks surprisingly high-def, but ultimately sort of useless as is, we see some light spots though. Next step we make a heatmap of only those topics we are interested in.


```{r, eval=TRUE}
topic_assignment3 <- topic_assignment2 %>% 
  select(`21`, `30`, `42`, `52`, `43`)

rouncor2 <- round(cor(topic_assignment3), 2)

melted2 <- melt(rouncor2)

ggplot(melted2, aes(x = as.character(X1), y = as.character(X2), fill = value)) + 
  geom_tile() + 
  theme_bw() + 
  labs(title = "Correlation map between topic Gammas", x = "", 
       y = "")


```

Quite low charges on all, save for topic 30, which performs best. The clear thing take away, especially in regards to education is that the MPs are not actively using the UN in talking about development, nor in terms of sustainability. A clear goal for the future lobbying should be to attempt to make sure that 4.7 becomes its own topic, together with keywords like education. Unfortunately, our model does not find this. Primarily because 4.7 appears quite rarely in the questions. 

```{r, eval=TRUE}
table(questiontext$session_id[which(str_detect(questiontext$text_a_l, "delmål 4.7")| 
      str_detect(questiontext$text_a_l, "bærekraftsmål 4") |
      str_detect(questiontext$text_a_l, "utdanning for bærekraftig utvikling") |
      str_detect(questiontext$text_q_l, "delmål 4.7")| 
      str_detect(questiontext$text_q_l, "bærekraftsmål 4") |
      str_detect(questiontext$text_q_l, "utdanning for bærekraftig utvikling") |
      str_detect(questiontext$justification_l, "delmål 4.7")| 
      str_detect(questiontext$justification_l, "bærekraftsmål 4") |
      str_detect(questiontext$justification_l, "utdanning for bærekraftig utvikling"))])

```

As we can see, we see only 4 mentions of sustainable development related keyphrases in our set of questions. This is despite the fact that we see quite a lot more mentions of education in general. 

```{r, eval=TRUE}
table(questiontext$session_id[which(str_detect(questiontext$text_a_l, "utdanning") |
                                      str_detect(questiontext$text_q_l, "utdanning") |
                                      str_detect(questiontext$justification_l, "utdanning"))])
```

On average, about 200 questions are asked every session about education, but none of these speak of sustainable development, or about the UNs sustainability goals. 




# Budget

*Before we begin, allow me to paint you the full picture...*

In the process of retrieving mentions of some of the suborganisations in the decisions, we quickly found that the decisions that mention this largely consists of budgets. This piqued our interest as we could retrieve the money that is allocated to some of the organisations of the UN over time, thereby meeting a requirement of "quantitative analysis" on the activities of the UNA, given that their main priority is securing funding for the amazing organisations that they work with. 

Below is a hidden chunk which creates a new function, because the get_case function from Stortingscrape did not perform as expected. It creates the function get_case_fancey, which does the same as get_case, but it actually works.
```{r, eval=TRUE, echo=FALSE, include=FALSE}
get_case_fancey <- function (caseid = NA, good_manners = 0) 
{
  url <- paste0("https://data.stortinget.no/eksport/sak?sakid=", 
                caseid)
  base <- GET(url)
  resp <- http_type(base)
  if (resp != "text/xml") 
    stop(paste0("Response of ", url, " is not text/xml."), 
         call. = FALSE)
  status <- http_status(base)
  if (status$category != "Success") 
    stop(paste0("Response of ", url, " returned as '", status$message, 
                "'"), call. = FALSE)
  tmp <- read_html(base)
  tmp2 <- list(root = data.frame(response_date = tmp %>% html_elements("detaljert_sak > respons_dato_tid") %>% 
                                   html_text(), version = tmp %>% html_elements("detaljert_sak > versjon") %>% 
                                   html_text(), document_group = tmp %>% html_elements("detaljert_sak > dokumentgruppe") %>% 
                                   html_text(), finalized = tmp %>% html_elements("ferdigbehandlet") %>% 
                                   html_text(), reference = tmp %>% html_elements("henvisning") %>% 
                                   html_text(), id = tmp %>% html_elements("detaljert_sak > id") %>% 
                                   html_text(), req_text = tmp %>% html_elements("innstillingstekst") %>% 
                                   html_text(), committee_id = ifelse(identical(tmp %>% 
                                                                                  html_elements("komite > id") %>% html_text(), character()), 
                                                                      "", tmp %>% html_elements("komite > id") %>% html_text()), 
                                 title_short = tmp %>% html_elements("korttittel") %>% 
                                   html_text(), decision_short = tmp %>% html_elements("kortvedtak") %>% 
                                   html_text(), parenthesis_text = tmp %>% html_elements("parentestekst") %>% 
                                   html_text(), case_number = tmp %>% html_elements("sak_nummer") %>% 
                                   html_text(), session_id = tmp %>% html_elements("sak_sesjon") %>% 
                                   html_text(), proceedings_id = tmp %>% html_elements("saksgang > id") %>% 
                                   html_text(), proceedings_name = tmp %>% html_elements("saksgang > navn") %>% 
                                   html_text(), status = tmp %>% html_elements("detaljert_sak > status") %>% 
                                   html_text(), title = tmp %>% html_elements("detaljert_sak > tittel") %>% 
                                   html_text(), type = tmp %>% html_elements("detaljert_sak > type") %>% 
                                   html_text(), decision_text = tmp %>% html_elements("detaljert_sak > vedtakstekst") %>% 
                                   html_text()), topic = data.frame(is_main_topic = tmp %>% 
                                                                      html_elements("emne > er_hovedemne") %>% html_text(), 
                                                                    main_topic_id = tmp %>% html_elements("emne > hovedemne_id") %>% 
                                                                      html_text(), id = tmp %>% html_elements("emne > id") %>% 
                                                                      html_text(), navn = tmp %>% html_elements("emne > navn") %>% 
                                                                      html_text()), publication_references = data.frame(export_id = tmp %>% 
                                                                                                                          html_elements("publikasjon_referanse > eksport_id") %>% 
                                                                                                                          html_text(), link_text = tmp %>% html_elements("publikasjon_referanse > lenke_tekst") %>% 
                                                                                                                          html_text(), link_url = tmp %>% html_elements("publikasjon_referanse > lenke_url") %>% 
                                                                                                                          html_text(), type = tmp %>% html_elements("publikasjon_referanse > type") %>% 
                                                                                                                          html_text(), subtype = tmp %>% html_elements("publikasjon_referanse > undertype") %>% 
                                                                                                                          html_text()), proceeding_steps = data.frame(step_name = tmp %>% 
                                                                                                                                                                                                               html_elements("saksgang_steg > navn") %>% html_text(), 
                                                                                                                                                                                                             step_number = tmp %>% html_elements("saksgang_steg > steg_nummer") %>% 
                                                                                                                                                                                                               html_text(), outdated = tmp %>% html_elements("saksgang_steg > uaktuell") %>% 
                                                                                                                                                                                                               html_text()), spokespersons = data.frame(mp_id = tmp %>% 
                                                                                                                                                                                                                                                          html_elements("saksordfoerer_liste > representant > id") %>% 
                                                                                                                                                                                                                                                          html_text(), party_id = tmp %>% html_elements("saksordfoerer_liste > representant > parti > id") %>% 
                                                                                                                                                                                                                                                          html_text(), sub_mp = tmp %>% html_elements("saksordfoerer_liste > representant > vara_representant") %>% 
                                                                                                                                                                                                                                                          html_text()), keywords = data.frame(keyword = tmp %>% 
                                                                                                                                                                                                                                                                                                html_elements("stikkord_liste > string") %>% html_text()))
  Sys.sleep(good_manners)
  return(tmp2)
}

```

Lets then get on to getting the decisions that turn out to be about the UN activities. 

```{r, eval=FALSE}

a <- list()

## For loop that fetches all of our fun stuff

for(x in unique(sessions_storting$id)){
  it <- 100*(which(unique(sessions_storting$id) == x) / length(unique(sessions_storting$id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  
  a[[x]] <- get_session_decisions(sessionid = x, good_manners = 0)
}

all_decisions <- do.call("rbind", a)

rm(a)

## Lets filter it all out (this is not done properly because this is just 
## showing how to accidentally stumble across something usable)

UN_decisions <- all_decisions %>% 
  mutate(decision_text_l = str_to_lower(decision_text),
         title_text_l = str_to_lower(decision_title)) %>% 
  filter(str_detect(decision_text_l, "fn-sambandet") | 
           str_detect(title_text_l, "fn-sambandet") |
           str_detect(decision_text_l, "utdanning for bærekraftig utvikling") |
           str_detect(title_text_l, "utdanning for bærekraftig utvikling")| 
           str_detect(decision_text_l, "unicef") |
           str_detect(decision_text_l, "wfp") |
           str_detect(decision_text_l, "unaids") |
           str_detect(title_text_l, "unicef") |
           str_detect(title_text_l, "wfp") | 
           str_detect(title_text_l, "unaids"))

## Omg its like only 15 stuffs, but what are they?? lets get the case

a<-list()

for (x in UN_decisions$case_id) {
  it <- 100*(which(unique(UN_decisions$case_id) == x) / length(unique(UN_decisions$case_id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  
  try(a[[x]] <- get_case_fancey(caseid = x, good_manners = 0))
}

### we got the decisions, wooo.

### why is this here? who knows? its not used later, do with iteth whateth 
### thoueth pleatheth. 

b <- list()

for (x in unique(UN_decisions$case_id)) {
  b[[x]] <- a[[x]][["root"]]
}

### This seems to capture all budgets, which we can still use. 

UN_cases <- do.call("rbind", b)

save(UN_cases, file = "Budget_Analysis/UN_decisions.Rdata")
save(UN_cases, file = "Budget_Analysis/UN_cases.Rdata")

```

Inside the decision text we find the full budgets relevant to our area. By applying the magix of **regex** we can discover hitherto unknown treasures of information. (meaning we can find how much money each org gets every year, which would probably take us about 15 minutes to do manually, but thanks to the magic of automation we can turn a 15 minute task into a 3 day task, because we're programmers)

```{r, eval=TRUE}

load("Budget_Analysis/UN_decisions.Rdata")
load("Budget_Analysis/UN_cases.Rdata")


## First i write a new function to retrieve strings next to the target words
## we are interested in. 

grab_text <- function(text, target, before, after){
  min <- which(unlist(map(str_split(text, "\\s"), ~grepl(target, .x))))-before
  max <- which(unlist(map(str_split(text, "\\s"), ~grepl(target, .x))))+after
  
  paste(str_split(text, "\\s")[[1]][min:max], collapse = " ")
}

## The text has been so dirty, we need to give it a good clean ;)

for (i in 1:length(UN_decisions$decision_text_l)) {
  tmp1 <- read_html(UN_decisions$decision_text_l[i])
  
  tmp2 <- html_text(tmp1)
  
  UN_decisions$cleanedtext_l[i] <- str_squish(str_remove_all(tmp2, "\r\n")) 
  
}

## Lets make som empty lists, two sets of them even. 

unicefbudget <- list()
wfpbudget <- list()
unaidsbudget <- list()
undpbudget <- list()

unicefbudgetn <- list()
wfpbudgetn <- list()
unaidsbudgetn <- list()
undpbudgetn <- list()

## Empty, like my skull

### Lets look at what UNICEF gets 

for (x in 1:length(UN_decisions$cleanedtext_l)) {
  if(str_detect(UN_decisions$cleanedtext_l[x], "unicef")){
  unicefbudget[[x]] <- grab_text(text = UN_decisions$cleanedtext_l[x], 
                                 target = "unicef", before = 1, after  = 14)
  
  if(str_detect(unicefbudget[[x]], "reduseres")){
    tmpred <- grab_text(text = unicefbudget[[x]], target = "til", before = 0, after = 4)
    tmp1 <- str_extract(tmpred, "[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{1,3}")
    unicefbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
  }else{
  
  tmp1 <- str_extract(unicefbudget[[x]], 
                      "([:digit:]{1,3}\\s?)[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{3}")
  
  unicefbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
  }
  }else{next}
}


unicefbudgets_t <- do.call("rbind", unicefbudgetn)

unicefbudget_f <- data.frame(Year = c(2022,2021,2020,2019,2018,2017,
                                      2016,2015,2014,2013,2012,2011),
                             UnicefMoney = unicefbudgets_t)

ggplot(unicefbudget_f, aes(x = Year, y = UnicefMoney)) + 
  geom_point() + 
  geom_line() + 
  theme_bw() + 
  labs(title = "UNICEF grants by year", y = "NOK", x = "") + 
  scale_x_continuous(breaks = c(2011, 2012, 2013, 2014, 2015, 2016,
                                2017, 2018, 2019, 2020, 2021, 2022))


```

By the looks of this, I would say that SV in government is associated with more money for UNICEF. 

Lets look at WFP.

```{r, eval=TRUE}
for (x in 1:length(UN_decisions$cleanedtext_l)) {
  if(str_detect(UN_decisions$cleanedtext_l[x], "wfp")){
    wfpbudget[[x]] <- grab_text(text = UN_decisions$cleanedtext_l[x], 
                                target = "wfp", before = 1, after  = 14)
    if(str_detect(wfpbudget[[x]], "reduseres")){
      tmpred <- grab_text(text = wfpbudget[[x]], target = "til", before = 0, after = 4)
      tmp1 <- str_extract(tmpred, "[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{1,3}")
      wfpbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
    }else{
      
      if(str_detect(wfpbudget[[x]], "auka")){
      tmpinc <- grab_text(text = UN_decisions$cleanedtext_l[x], 
                          target = "wfp", before = 1, after = 20)
      tmpinc2 <- grab_text(text = tmpinc, target = "til", before = 0, after = 5)
      tmpinc3 <- str_extract(tmpinc2, "[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{1,3}")

      wfpbudgetn[[x]] <- (as.numeric(str_remove_all(tmpinc3, " ")))
      
      }else{

      tmp1 <- str_extract(wfpbudget[[x]], "([:digit:]{1,3}\\s?)[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{3}")

      wfpbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
    }}
  }else{next}
}


wfpbudget_t <- do.call("rbind", wfpbudgetn)

wfpbudget_f <- data.frame(Year = c(2022,2021,2020,2019,2018,2017,
                                      2016,2015,2014,2013,2012,2011),
                             WFPMoney = wfpbudget_t)

ggplot(wfpbudget_f, aes(x = Year, y = WFPMoney)) + 
  geom_point() + 
  geom_line() + 
  theme_bw() + 
  labs(title = "WFP grants by year", y = "NOK", x = "") + 
  scale_x_continuous(breaks = c(2011, 2012, 2013, 2014, 2015, 2016,
                                2017, 2018, 2019, 2020, 2021, 2022))


```

Looks like the peace prize was helpful.

Next we look at UNDP. 

```{r, eval=TRUE}

for (x in 1:length(UN_decisions$cleanedtext_l)) {
  if(str_detect(UN_decisions$cleanedtext_l[x], "undp")){
    undpbudget[[x]] <- grab_text(text = UN_decisions$cleanedtext_l[x], 
                                 target = "undp", before = 1, after  = 14)
    if(str_detect(undpbudget[[x]], "reduseres")){
      tmpred <- grab_text(text = undpbudget[[x]], target = "til", before = 0, after = 4)
      tmp1 <- str_extract(tmpred, "[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{1,3}")
      undpbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
    }else{
      
      if(str_detect(undpbudget[[x]], "auka")){
        tmpinc <- grab_text(text = UN_decisions$cleanedtext_l[x], 
                            target = "undp", before = 1, after = 20)
        tmpinc2 <- grab_text(text = tmpinc, target = "til", before = 0, after = 5)
        tmpinc3 <- str_extract(tmpinc2, "[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{1,3}")
        
        undpbudgetn[[x]] <- (as.numeric(str_remove_all(tmpinc3, " ")))
        
      }else{
        
        tmp1 <- str_extract(undpbudget[[x]], 
                            "([:digit:]{1,3}\\s?)[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{3}")
        
        undpbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
      }}
  }else{next}
}


undpbudget_t <- do.call("rbind", undpbudgetn)

undpbudget_f <- data.frame(Year = c(2022,2021,2020,2019,2018,2017,
                                   2016,2015,2014,2013,2012,2011),
                          UNDPMoney = undpbudget_t)

ggplot(undpbudget_f, aes(x = Year, y = UNDPMoney)) + 
  geom_point() + 
  geom_line() + 
  theme_bw() + 
  labs(title = "UNDP grants by year", y = "NOK", x = "") + 
  scale_x_continuous(breaks = c(2011, 2012, 2013, 2014, 2015, 2016,
                                2017, 2018, 2019, 2020, 2021, 2022))

```

Looks like decreasing the budget is a popular thing. Maybe its not all mean old Støre that wants to reduce it. 

Lets finally have a gander at UNAIDS grants for every year.

```{r, eval=TRUE}

for (x in 1:length(UN_decisions$cleanedtext_l)) {
  if(str_detect(UN_decisions$cleanedtext_l[x], "unaids")){
    unaidsbudget[[x]] <- grab_text(text = UN_decisions$cleanedtext_l[x], 
                                   target = "unaids", before = 1, after  = 14)
    if(str_detect(unaidsbudget[[x]], "reduseres")){
      tmpred <- grab_text(text = unaidsbudget[[x]], 
                          target = "til", before = 0, after = 4)
      tmp1 <- str_extract(tmpred, "[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{1,3}")
      unaidsbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
    }else{
      
      if(str_detect(unaidsbudget[[x]], "auka")){
        tmpinc <- grab_text(text = UN_decisions$cleanedtext_l[x], 
                            target = "unaids", before = 1, after = 20)
        tmpinc2 <- grab_text(text = tmpinc, 
                             target = "til", before = 0, after = 5)
        tmpinc3 <- str_extract(tmpinc2, "[:digit:]{1,3}\\s[:digit:]{1,3}\\s[:digit:]{1,3}")
        
        unaidsbudgetn[[x]] <- (as.numeric(str_remove_all(tmpinc3, " ")))
        
      }else{
        
        tmp1 <- str_extract(unaidsbudget[[x]], 
                            "([:digit:]{1,3}\\s?)[:digit:]{1,3}\\s[:digit:]{1,3}\\s000")
        
        unaidsbudgetn[[x]] <- (as.numeric(str_remove_all(tmp1, " ")))
      }}
  }else{next}
}
UN_decisions$decision_title
unaidsbudget_t <- do.call("rbind", unaidsbudgetn)

unaidsbudget_f <- data.frame(Year = c(2021,2020, 2019, 2018.5,2018,2017,
                                    2016, 2015.5,2015,2014,2013,2012,2011),
                           UNAIDSMoney = unaidsbudget_t)

ggplot(unaidsbudget_f, aes(x = Year, y = UNAIDSMoney)) + 
  geom_point() + 
  geom_line() + 
  theme_bw() + 
  labs(title = "UNAIDS grants by year", y = "NOK", x = "") + 
  scale_x_continuous(breaks = c(2011, 2012, 2013, 2014, 2015, 2016,
                                2017, 2018, 2019, 2020, 2021, 2022))

```

Lots of supplementary budgets here. Weird that the negotiations in 2018 lead to a decrease. 

Note that for all of these, Støres 2021 budget is coded as 2022, purely out of convenience. 

Overall this provides a possibility of overviewing the budgets of the UN organizations, that are gathered in an automated manner. Although, given the detailed regex controls it is arguable whether it is truly "automatic", nonetheless it can be reused in the future to gather even more data simply by adding the session in the start of the code.

# Hearings

This is a step by step guide to scraping the hearings for info on FN-Sambandet,
and related priority words


```{r, eval= TRUE}
#library in relevant packages
library(tidyverse)
library(tidytext)
install.packages("quanteda")
library(quanteda)
install.packages("stm")
library(stm)
library(tidyr)


#library in the stortingpackage
library(stortingscrape)

#there are three relevant functions for hearings. With the questionmark we learn more about them
?get_session_hearings()
?get_hearing_input()
?get_written_hearing_input()


#before running one of these functions, we want to get all parliament sessions and specify that we only scrape from 2011 onwards
#therefore we create an object for the sessions and get all parliament sessions
sessions_storting <- get_parlsessions()

#as you now see, the sessions appear as an object in your environment. Let's filter out the relevant years
#filter out the sessions after 2011, which have relevant data
sessions_storting <- sessions_storting %>%
  filter(id %in% c("2011-2012", "2012-2013", "2013-2014",
                   "2014-2015", "2015-2016", "2016-2017",
                   "2017-2018", "2018-2019", "2019-2020", 
                   "2020-2021", "2021-2022"))

# now we are creating three lists to fill in information afterwards. We create three lists, as we have three relevant functions to run.
a <- list()
b <- list()
c <- list()

#you can now see the three empty lists in your environment, let's fill them

#now we can run a for loop for all three relevant functions

#We need to start with get_session_hearings, as we need hearing ids for the other functions.
#run a for-loop to retrieve session hearings. I also added the progress bar
for(x in unique(sessions_storting$id)){
  it <- 100*(which(unique(sessions_storting$id) == x) / length(unique(sessions_storting$id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  a[[x]] <- get_session_hearings(sessionid = x, good_manners = 0, cores = 1)
}

#place the retrieved data from list a in a new object
allsessionshearings <- do.call(rbind, a)

?unnest
#unnesting the data into columns that we can work with
allsessionshearings <- allsessionshearings %>% as_tibble() %>% 
  unnest(cols = root, hearing, hearing_case_info, hearing_date )

#no we find a large dataset with a lot of variables and a good overview over hearings in our allsessionhearings object
#as we now know the hearing ids, we can run the other two functions

?get_hearing_input
#First, running a loop to retrieve hearing inputs. I am amending the loop according to where the hearing id is found and what arguments the function demands me to specify.
#I am also directing the data towards list b.
#as it returns an error message, we use the "try" function
for(x in unique(allsessionshearings$hearing_id)){
  it <- 100*(which(unique(allsessionshearings$hearing_id) == x) / length(unique(allsessionshearings$hearing_id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  try(b[[x]] <- get_hearing_input(hearingid = x, good_manners = 0), silent = TRUE)
}

#we are observing that data from before the 2020-2021 sessions is missing
#we continue with the input data we can extract

#this is just a test for the last hearing that returns data
get_hearing_input(hearingid = "10004183")

#place the retrieved data from list b into another object we can work with
hearinginput <- do.call(rbind, b)

#unnesting again. I can choose which variables are relevant and leave the rest out. In case I want to include e.g. response date at a later stage, I have to amend the code here.
#You can check all variables in the hearinginput object
hearinginput <- hearinginput %>% as_tibble() %>% 
  unnest(cols = hearing_id, committee_id, hearing_input_organization, hearing_input_title, hearing_input_text)

?get_written_hearing_input
#Second, running a loop to retrieve written hearing input. Remember amending for the data to be put in list c.
#For some reason, the loop only returns NA values - quickly stop the execution of the loop!! Need to investigate why! Maybe many hearings don't have written hearing input? As we learned by emails, the written hearing input lies in archives and needs a request to be opened.
#or the function is not really relevant as it only retrieves the same variables as get_hearing_input (input text and title most relevant)
for(x in unique(allsessionshearings$hearing_id)){
  it <- 100*(which(unique(allsessionshearings$hearing_id) == x) / length(unique(allsessionshearings$hearing_id)))
  cat(paste0(sprintf("Progress: %.4f%%             ", it), "\r"))
  c[[x]] <- get_written_hearing_input(hearingid = x, good_manners = 0)
}

#place retrievend data in list c
written_hearing_input <- do.call(rbind, c)

#unnesting data
written_hearing_input <- written_hearing_input %>% as_tibble() %>% 
  unnest(cols = hearing_id, committee_id, hearing_input_organization, hearing_input_title, hearing_input_text)

#let's save the dataset we have obtained before continuing
#You can use getwd() to locate your current folder and create a path
getwd()
save(allsessionshearings, file = "cleaned docs/allsessionhearings.RData")
save(hearinginput, file = "cleaned docs/hearinginput.RData")
save(written_hearing_input, file = "cleaned docs/written_hearing_input.RData")

load(file = "cleaned docs/allsessionhearings.RData")
load(file = "cleaned docs/hearinginput.RData")
load(file = "cleaned docs/written_hearing_input.RData")
#for now I will continue especially with the data from hearinginput, which has the hearing title and text to see where UNA is mentioned

#loading relevant packages again
library(tidyverse)

#creating a new object by selecting relevant variables and merging text and titles of hearings into one variable
hearingtext <- hearinginput %>% 
  select(hearing_id, hearing_input_organization, committee_id, hearing_input_date, hearing_input_text, hearing_input_title) %>% 
  mutate(fulltext = paste(hearing_input_title, hearing_input_text))

#Test: now we can use the fulltext variable to check for FN sambandet and other keywords
mentionsFNS <- hearingtext %>% 
  filter(str_detect(fulltext, "FN-sambandet"))
#we observe three mentions in hearings

#In a more professional manner we first homogenize the text to make our search easier
library(stringr)

#changing all text to lower case and removing html patterns
hearingtext <- hearingtext %>% 
  mutate(fulltext_lo = str_remove_all(str_to_lower(fulltext), pattern = "<p>"))

#removing all text that is written in <> parentheses (leftover html code)
hearingtext <- hearingtext %>% 
  mutate(fulltext_lo = str_remove_all(fulltext_lo, pattern = "<.*>"), 
         fulltext_lo = str_replace_all(fulltext_lo, pattern =  "\\s*\\([^\\)]+\\\\", ""))

#Test: we can now check for keywords
sum(str_detect(hearingtext$fulltext_lo,"fn-sambandet"))
#1 results 
#we observe less mentions here, as only hearing text and title are searched (FN Sambandet as the hearing organisation is omitted)

sum(str_detect(hearingtext$fulltext_lo, "utdanning for bærekraftig utvikling"))
#0 results

#this next code prints all the text input that contains "4.7"
hearingtext$fulltext[str_which(hearingtext$fulltext_lo, "4.7")]

sum(str_detect(hearingtext$fulltext,"fn-sambandet"))
#O results

#we also check for mentions of related organizations

sum(str_detect(hearingtext$fulltext, "utdanning for bærekraftig utvikling"))
#1 results

sum(str_detect(hearingtext$fulltext, "wfp"))
# 2 results

sum(str_detect(hearingtext$fulltext, "undp"))
# 2 results

sum(str_detect(hearingtext$fulltext, "unesco"))
#20 results

sum(str_detect(hearingtext$fulltext, "who"))
# 50 results

sum(str_detect(hearingtext$fulltext, "ilo"))
# 21 results

sum(str_detect(hearingtext$fulltext, "un-habitat"))
# 0 results

sum(str_detect(hearingtext$fulltext, "unicef"))
# 18

sum(str_detect(hearingtext$fulltext, "UNEP"))
# 6 reulsts

sum(str_detect(hearingtext$fulltext, "sdg 4"))
# results 1

sum(str_detect(hearingtext$fulltext, "god utdanning")) #Name of a specific goal, yet to broad and heavily used outside of the specific title of the goal
# 19 results


#this string detects whether or not an input contains "samfunnsøokonomisk"
str_detect(hearingtext$fulltext_lo,"samfunnsøkonomisk")
# and this one counts the mentions again
sum(str_detect(hearingtext$fulltext_lo, "samfunnsøkonomisk"))
#53 mentions

#As we see there is a high level of discrepancy between different mentions depending on how related or not they are to our specific goal - Thus, we have prioritized, depending on relevance. And labeled according to three priority levels. 

#Now we prepare plotting by creating new variables that unite all the keyword searches after priority (1-3)
keywordmentions <- hearingtext %>% 
  mutate(priority_level = case_when(
    str_detect(fulltext_lo, "fn-sambandet") ~ "1",
    str_detect(fulltext_lo, " una ") ~ "1", #find another funtion as this find all words that include "una"
    str_detect(fulltext_lo, "united nations association") ~ "1",
    str_detect(fulltext_lo, "fn sambandet") ~ "1",
    str_detect(fulltext_lo, " ubu ") ~ "2",
    str_detect(fulltext_lo, "utdanning for bærekraftig utvikling") ~ "2",
    str_detect(fulltext_lo, "utdanning i bærekraftig utvikling") ~ "2",
    str_detect(fulltext_lo, "utdanning om bærekraftig utvikling") ~ "2",
    str_detect(fulltext_lo, "education for sustainable development") ~ "2",
    str_detect(fulltext_lo, "education for sustainability") ~ "2",
    str_detect(fulltext_lo, " esd ") ~ "2",
    str_detect(fulltext_lo, "delmål 4.7") ~ "2",
    str_detect(fulltext_lo, "mål 4.7") ~ "2",
    str_detect(fulltext_lo, "bærekraftsmål 4") ~ "2",
    str_detect(fulltext_lo, " sdg 4 ") ~ "2",
    str_detect(fulltext_lo, " unicef ") ~ "3",
    str_detect(fulltext_lo, " ilo ") ~ "3",
    str_detect(fulltext_lo, " unesco ") ~ "3",
    str_detect(fulltext_lo, " who ") ~ "3",
    str_detect(fulltext_lo, " wfp ") ~ "3",
    str_detect(fulltext_lo, " undp ") ~ "3",
    str_detect(fulltext_lo, " unep ") ~ "3",
    str_detect(fulltext_lo, " fao ") ~ "3",
  ))
#keywordmentions includes NAs and is therefore not fit for presentation

  
#therefore we create a subset that only contains the rows with relevant mentions
keyword_subset <- keywordmentions %>% 
  filter(str_detect(fulltext_lo, "fn-sambandet")|
           str_detect(fulltext_lo, " una ")| #find another funtion as this find all words that include "una"
           str_detect(fulltext_lo, "united nations association")|
           str_detect(fulltext_lo, "fn sambandet")|
           str_detect(fulltext_lo, " ubu ")|
           str_detect(fulltext_lo, "utdanning for bærekraftig utvikling")|
           str_detect(fulltext_lo, "utdanning i bærekraftig utvikling")|
           str_detect(fulltext_lo, "utdanning om bærekraftig utvikling")|
           str_detect(fulltext_lo, "education for sustainable development")|
           str_detect(fulltext_lo, "education for sustainability")|
           str_detect(fulltext_lo, " esd ")|
           str_detect(fulltext_lo, "delmål 4.7")|
           str_detect(fulltext_lo, "mål 4.7")|
           str_detect(fulltext_lo, "bærekraftsmål 4")|
           str_detect(fulltext_lo, " sdg 4 ")|
           str_detect(fulltext_lo, " ilo ")|
           str_detect(fulltext_lo, " unesco ")|
           str_detect(fulltext_lo, " who ")|
           str_detect(fulltext_lo, " fao ")|
           str_detect(fulltext_lo, " wfp ")|
           str_detect(fulltext_lo, " undp ")|
           str_detect(fulltext_lo, " unep ")|
           str_detect(fulltext_lo, " unicef "))
  
#now we create a full subset called "subfullset" in which we include hearing ids beside the keyword_subset variables
#changing the full subset to contain hearing ids and session ids selected from allsessionhearings dataset
?left_join
subfullset <- allsessionshearings %>% 
  select(hearing_id, session_id)

#now we join the full subset together with the keyword_subset
keyword_subset <- left_join(keyword_subset, subfullset, by = c("hearing_id"))

#this table is just to 
table(allsessionshearings$session_id)


#with newkeywordmentions containing session ids and keywordmentions we can now plot

#now we are trying to plot these variables
ggplot(keywordmentions, aes(x = committee_id, fill = priority_level)) + 
  geom_bar() + 
  theme_bw()
#keywordmentions includes NAs and is therefore not fit for presentation


#creating a color palette for colorblind people as an object for later use
safe_colorblind_palette <- c("#88CCEE", "#CC6677", "#DDCC77", 
                             "#117733", "#332288", "#AA4499", 
                             "#44AA99", "#999933", "#882255", 
                             "#661100", "#6699CC", "#888888")

#plotting only the relevant keyword mentions after priority
ggplot(keyword_subset, aes(x = session_id, fill = priority_level)) + 
  geom_bar(position = position_dodge(width = 0.8)) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "Keyword Mentions in Hearings",
       x = "Storting Sessions",
       y = "Number of Mentions",
       fill = "Priority level",
       caption = "Mentions after priority level: 
       1 - direct mentions of UNA and related terms 
       2 - mentions of terms related to SDG 4.7 on education
       3 - mentions of associated UN Organisations") +
  theme(legend.position = "right", legend.fill = "Priority Level",
        plot.caption.position = "plot") +
  theme_light() +
  coord_cartesian(ylim = c(0,5))

save(keyword_subset, file = "C:/Users/hanne/Desktop/Coding R and Python/summer course/Group-3-ISSSV1337/cleaned docs/keyword_subset.Rdata")
load(file = "cleaned docs/keyword_subset.Rdata")
#further analysis



```



# Social Media

# Decisions

# Keywords


Vizualisation of search words/related words

When creating a list of the different search words/possible relevant words I found it

practical to work in Excel as part of the development and revision of the table

The results can be converted into R format, using packages that read Excel files and the 
formatting in Excel
xlsx format readed by the function

Synonymordtabell <- read_excel("C:/Users/marku/OneDrive - Universitetet i Oslo/Synonymordtabell - kopi.xlsx")
NA simply means empty cells in the Excel documents, as there were different number of words under each of the categories


```{r, out.width = "80%", fig.align = "center"}
knitr::include_graphics("C:/Users/marku/Downloads/Excel III.jpg")
```


```{r, out.width = "80%", fig.align = "center"}
knitr::include_graphics("C:/Users/marku/Downloads/R.verson of table.jpg")
```


View(Synonymordtabell)

Under the “Eksempler på utfordringen med søkeord»-column in the dataset/table, some examples are highlighted that also applies to many of the other words in the table. 
The challenge when searching for words is partly that there exists so many synonyms that are identical or similar in content. Different actors in politics in general incl. Stortinget may say or refer to the same (possibly issues related to FN-sambandet) in different ways. Also, related terms might seem tied to the UNAs work or interest, while in reality simply being catchy political terms without connection to the goal of this study.
•	If I enter search word «utdanning for bærekraftig utvikling» I will get results where for is used, but not results where the politicians or people have used “utdanning om bærekraftig utvikling” or “utdanning i bærekraftig utvikling”
-	Utdanning for bærekraftig utvikling
-	Utdanning i bærekraftig utvikling
-	Utdanning om bærekraftig utvikling
-	UBU
 
•	In terms of related UN organisations like WFP one can search for 
-	WFP
-	World Food Programme
-	FNs matvareprogram 

•	The same applies to ILO: 
-	ILO, International labor organization
-	international labour organization
-	international labor organisation
-	international labour organisation
-	den internasjonale arbeidslivsorganisasjonen.   

Will get different results in terms of mentions when varying these nuances in the search words 

FYI: has recoded terms in R so that searches can be done only with small letters, hence no discussion of the variation of capital and small letters, which in some search settings and applications could be an issue 
•	If not recoded R would treat search terms differently with and without capital letters. 

A challenge when using the search appliance at Stortinget’s website is that the number of occurrences of FN-sambandet and other organisations can represent when those organisations is the source of speech or text, rather than FN-sambandet being mentioned by others. The Stortingsscrape code will not include those occurrences wherein 



Another issue with applying the search words in our R code is that the Stortingsscrape-package we have based our solutions on cannot import skriftlige innspill (written inputs), so that some results are omitted. For example “utdanning for bærekraftig utvikling” and “delmål 4.7” was used by another actor than FN-Sambandet in a written input from Naturfagsenteret v/ Universitetet i Oslo”. Possibly, that is a challenge in other applications if for example many usages of words occurred in such written inputs. 


In the end we chose to not apply all possible search words due to shortage of time
To run the codes searching through vast amounts of documents in Stortinget will take more time the more words and word inflections that were used. Below is a picture with the search words we chose to apply in our R codes. Note that they are only a subset of the words shown in the preceding Excel-overview of the search words. 

The priority of words, as reflected in the various plots, are shown by numbers, wherein 1 stands for the highest priority of words. First prirority was how often FN-sambandet was used, educational terms (utdanning for bærelraftig utvikling etc..), and priority3 reflects other related terms. We stopped short of exploring all possibly related terms that could reside inside priority 2 and 3 due to the restraints of time and machine capacity in running the elaborate codes. See below for the terms we used in the codes: 




```{r, out.width = "80%", fig.align = "center"}
knitr::include_graphics("C:/Users/marku/Downloads/Illustrasjon i R med valgte søkeord.jpg")
```
# Scraping facebook and Instagram
## - A guide with code

Due to the strict privacy policies of Facebook, one must have admin access to the pages one wish to scrape. Since we do not have access to any of FN-sambandets social media accounts, we have structured the code with notes on what they do, to be used by the admin of these accounts.


When you have created an app through [Meta for developer](https://developers.facebook.com/), which is connected to a  facebook account containing admin access to a public facebook page, you will be able to access your token. The token is the key to accessing the Facebook API.


Below are listed to two packages we will use 
```{r, eval=FALSE}
library(Rfacebook)
library(instaR)
```
*Functions used in this guide is ones that are most useful for FN-sambandet.*  
*For full list of functions from these packages see [Rfacebook](https://cran.r-project.org/web/packages/Rfacebook/Rfacebook.pdf) and [InstaR](https://cran.r-project.org/web/packages/instaR/instaR.pdf).*

## Facebook

You can find you temporary token in the "Graph API explorer" under "tools". 
These tokens expires after two hours. 
*Note: The token included is just to show of the token will look like. It is not functional.*

```{r, eval=FALSE}
Token <- "EAAGEubsXctYBAOT1lUatovZCBs0GFQZA5ml43Dnh5k3lIDa0UmDm8qMuxAMaZCtStodH0HBBgwzBR1CniWnAnLZAyKHAzdKabnnw5EiX4pZBFwg38TYCnlSUtGegWYbSdtJX79YJ8bT8eElFr5OWheiAXmmdr48HyTBIA2JTggFZCb11Y1iYzvpeUnZA3V3DQx6J15AcaqrtQZDZD"
```

This code creates two values, the first for todays date, and the second for the date exactly one year ago. By using these we are able to select posts and interactions that have occured within the last year. This can of course be changes to the desired values. 

```{r, eval=FALSE}
date1<-Sys.Date()-365
date2<-Sys.Date()
```


What would be the most useful code for scraping data on interactions would be getInsights. This retrieves information from the 
```{r, eval=FALSE}
Insights <- getInsights(object_id='FN-sambandet', #This could also be switched out with a id of a spesicif post.
                        token = Token, 
                        metric= c('page_engaged_users','page_impressions'), #Here we type what exactly we want to collect. To see full list of which metrics you are able to spesify here check out: https://developers.facebook.com/docs/graph-api/reference/v14.0/insights
                        period = c('lifetime','day'),
                        parms = "&since=date1&until=date2") #choosing to scrape from the last year.
```

GetReactions retrives the reactions users had to one or more post. getShares retrives post containg a sharing of one or more of your posts. Collecting this data could be useful to examine posts that have received a larger number of negative response, and  
```{r, eval=FALSE}
Reactions <- getReactions(post = "https://fb.watch/eG7vqzZLUj/", #link to the post we wish to scrape. This is FN-sambandets current pinned post.
             token = Token,
             verbose = TRUE, api = NULL)
Shares <- getShares(post = "https://fb.watch/eG7vqzZLUj/", #Again using the pinned post.
                    token = Token,
                    n = 100) #choosing to get the first 100 shares.
```



## Instagram

The idea behind scraping instagram is the same as scraping facebook. This would allow you to track and measure engagements on the post you 

The process of obtain the token needed to scrape your Instagram data, is quite similar as obtaining the facebook token. 
```{r, eval=FALSE}
TokenInsta <- "IGQVJWS2VLOUoxbFVhZAWd2bTU4TU4xY2ZApRVJ0WDM3akFuWnV5OGtwSXphVFVaTE5yREJTUUY0ZAHpBRVJQdThXX1RlcGhuOUk0TExvS3NaSklKV0V2SHRRei00Q01YX0Y5MTRSUHYtaC1SdUNobS1BWQZDZD"
```

getFollowers retrives a list of followers, and put it in a dataframe. This data could potentially be used to crosscheck with members of parlament, or potential  *influencers* whos could bring awareness to your cause. 
```{r, eval=FALSE}
Followers <- getFollowers(username = "fnsambandet",
                        token = TokenInsta , 
                        userid = NULL, 
                        verbose = TRUE)
```


The function getComments to retrieve comments from one or more post, and place them in a dataset. 
```{r, eval=FALSE}
FNS <- getUserMedia(username = "fnsambandet",
                    token = TokenInsta) #This saves your profile as an object, making it easier to implement into other codes.
comments <- getComments(FNS$id[Cgv3mNjqhvk], #This code within the brackets are the ID of a randome IG-post from FN-samabndet. You will find this in the link when using IG in a browser.
            token = TokenInsta, 
            verbose = TRUE)
```

The function getTagCount gives the number of times a hashtag has been used or commented on a post.
```{r, eval=FALSE}
Hashtags <- getTagCount(tag = "fnsambandet", #This could also be changed to try out different hashtags related to your work.
                        token = TokenInsta)
```


This walkthrough on how to scrape the social media profiles retrieves datasets for each function. These datasets would be used further to conduct a analysis of the engagement you receive on social media, and from whom. 




